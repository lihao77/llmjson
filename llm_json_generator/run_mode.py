#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM JSON Generator è¿è¡Œæ¨¡å¼

æä¾›ä¸åŒçš„æ–‡æ¡£å¤„ç†è¿è¡Œæ¨¡å¼ï¼ŒåŒ…æ‹¬æ‰¹é‡å¤„ç†å’Œä¼˜åŒ–æµå¼å¤„ç†ã€‚
ä»complete_document_processing_example.pyä¸­æå–çš„æ¨¡å¼1å’Œæ¨¡å¼3ã€‚
"""

import os
import json
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from .processor import LLMProcessor
from .config import ConfigManager
from .utils import (
    ensure_dir,
    save_json,
    load_json,
    Timer,
    sanitize_filename,
    merge_knowledge_graph_results
)
from .log import (
    setup_logging,
    get_logger,
    create_logger_with_context,
    create_timed_logger,
    log_execution_time,
    log_system_info
)
from .exceptions import LLMProcessingError, ValidationError
from .word_chunker import WordChunker
from .validator import DataValidator
from .utils import chunk_text
from .prompt_template import PromptTemplate

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ - æä¾›å¤šç§å¤„ç†æ¨¡å¼"""
    
    def __init__(self, config_path: str = "config.json", template_file: Optional[str] = None):
        """åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        
        self.config_path = config_path
        self.config = ConfigManager(config_path)
        
        # åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        self.logger = create_logger_with_context({
            'component': 'DocumentProcessor',
            'config_path': config_path,
            'template_file': template_file
        })
        
        # åˆå§‹åŒ–LLMå¤„ç†å™¨
        merged_config = self.config.get_merged_config()
        
        if template_file:
            self.prompt_template = PromptTemplate(template_file)
            self.llm_processor = LLMProcessor(**merged_config, prompt_template=self.prompt_template)
        else:
            self.llm_processor = LLMProcessor(**merged_config)
        
        # åˆå§‹åŒ–Wordåˆ†å—å™¨
        self.word_chunker = WordChunker(
            max_tokens=self.config.processing_config.chunk_size,
            overlap_tokens=self.config.processing_config.chunk_overlap
        )
        
        # åˆå§‹åŒ–éªŒè¯å™¨
        self.validator = DataValidator()
    
    @log_execution_time()
    def _scan_folder_for_documents(self, folder_path: str) -> List[str]:
        """æ‰«ææ–‡ä»¶å¤¹è·å–æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        supported_extensions = {'.txt', '.docx', '.doc'}
        doc_files = []
        
        try:
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_ext = Path(file_path).suffix.lower()
                    
                    if file_ext in supported_extensions:
                        doc_files.append(file_path)
                        self.logger.debug(f"ğŸ“„ æ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶: {file_path}")
            
            # æŒ‰æ–‡ä»¶åæ’åº
            doc_files.sort()
            self.logger.info(f"ğŸ“Š åœ¨æ–‡ä»¶å¤¹ {folder_path} ä¸­æ‰¾åˆ° {len(doc_files)} ä¸ªæ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰«ææ–‡ä»¶å¤¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
        
        return doc_files
    
    @log_execution_time()
    def process_document_list_batch(self, 
                                   folder_path: str,
                                   base_output_dir: str = "output",
                                   include_tables: bool = True,
                                   generate_validation_report: bool = True) -> Dict[str, Any]:
        """ä¼ ç»Ÿæ‰¹é‡å¤„ç† (ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ–‡æ¡£)
        
        Args:
            folder_path: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
            include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
            generate_validation_report: æ˜¯å¦ç”ŸæˆéªŒè¯æŠ¥å‘Š
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        # åˆ›å»ºå¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—å™¨
        batch_logger = create_logger_with_context({
            'mode': 'batch_processing',
            'folder_path': folder_path,
            'output_dir': base_output_dir,
            'include_tables': include_tables,
            'validation': generate_validation_report
        })
        
        start_time = time.time()
        
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            batch_logger.error(f"ğŸ“ æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}")
            return {
                'success': False,
                'error': f'æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}',
                'processing_time': 0
            }
        
        # æ‰«ææ–‡ä»¶å¤¹è·å–æ–‡æ¡£æ–‡ä»¶
        doc_paths = self._scan_folder_for_documents(folder_path)
        
        if not doc_paths:
            batch_logger.warning(f"ğŸ“ æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶: {folder_path}")
            return {
                'success': False,
                'error': 'æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶',
                'processing_time': 0
            }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(base_output_dir, f"batch_processing_{timestamp}")
        ensure_dir(output_dir)
        
        batch_logger.info(f"ğŸ“ æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        batch_logger.info(f"ğŸ“Š æ‰¾åˆ° {len(doc_paths)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        batch_logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
        
        document_results = {}
        
        # ä½¿ç”¨LLMProcessorçš„æ‰¹é‡å¤„ç†æ–¹æ³•
        try:
            batch_logger.info("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡æ¡£...")
            
            # ä½¿ç”¨processorçš„process_documentsæ–¹æ³•
            batch_results = self.llm_processor.process_documents(
                doc_paths, include_tables=include_tables
            )
            
            # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„ç»“æœ
            for doc_path, doc_chunk_results in batch_results.items():
                try:
                    doc_start_time = time.time()
                    batch_logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£ç»“æœ: {doc_path}")
                    
                    # è½¬æ¢ç»“æœæ ¼å¼ï¼Œä¿ç•™å®Œæ•´çš„processing_info
                    chunk_results = []
                    for i, (result, info) in enumerate(doc_chunk_results):
                        chunk_results.append({
                            'chunk_index': i,
                            'result': result if info['success'] else None,
                            'success': info['success'],
                            'error': info.get('error') if not info['success'] else None,
                            'processing_time': info.get('processing_time', 0),
                            # ä¿ç•™å®Œæ•´çš„processing_info
                            'full_info': info
                        })
                    
                    # å¤„ç†å•ä¸ªæ–‡æ¡£çš„ç»“æœ
                    doc_result = self._process_document_results(
                        doc_path, chunk_results, output_dir, generate_validation_report
                    )
                    
                    doc_result['processing_time'] = time.time() - doc_start_time
                    document_results[doc_path] = doc_result
                    
                    if doc_result['success']:
                        batch_logger.info(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: {doc_path}")
                    else:
                        batch_logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {doc_path} - {doc_result.get('error')}")
                        
                except Exception as e:
                    batch_logger.error(f"âŒ å¤„ç†æ–‡æ¡£ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {doc_path} - {e}")
                    document_results[doc_path] = {
                        'success': False,
                        'error': f"ç»“æœå¤„ç†å¤±è´¥: {e}",
                        'processing_time': 0
                    }
        
        except Exception as e:
            batch_logger.error(f"âŒ æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # ä¸ºæ‰€æœ‰æ–‡æ¡£æ·»åŠ é”™è¯¯è®°å½•
            for doc_path in doc_paths:
                if doc_path not in document_results:
                    document_results[doc_path] = {
                        'success': False,
                        'error': f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}",
                        'processing_time': 0
                    }
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        results = self._generate_final_results(
            document_results, output_dir, total_time, "batch_processing"
        )
        
        # æ˜¾ç¤ºç»“æœ
        self._display_results(results)
        
        return results
    
    @log_execution_time()
    def process_document_list_streaming_optimized(self,
                                                 folder_path: str,
                                                 base_output_dir: str = "output",
                                                 include_tables: bool = True,
                                                 generate_validation_report: bool = True) -> Dict[str, Any]:
        """ä¼˜åŒ–æµå¼å¤„ç† (æ‰¹é‡æµå¼å¤„ç†ï¼Œå……åˆ†åˆ©ç”¨çº¿ç¨‹èµ„æº)
        
        Args:
            folder_path: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
            include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
            generate_validation_report: æ˜¯å¦ç”ŸæˆéªŒè¯æŠ¥å‘Š
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        # åˆ›å»ºå¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—å™¨
        streaming_logger = create_logger_with_context({
            'mode': 'streaming_optimized',
            'folder_path': folder_path,
            'output_dir': base_output_dir,
            'include_tables': include_tables,
            'validation': generate_validation_report
        })
        
        start_time = time.time()
        
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
            streaming_logger.error(f"ğŸ“ æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}")
            return {
                'success': False,
                'error': f'æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {folder_path}',
                'processing_time': 0
            }
        
        # æ‰«ææ–‡ä»¶å¤¹è·å–æ–‡æ¡£æ–‡ä»¶
        doc_paths = self._scan_folder_for_documents(folder_path)
        
        if not doc_paths:
            streaming_logger.warning(f"ğŸ“ æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶: {folder_path}")
            return {
                'success': False,
                'error': 'æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶',
                'processing_time': 0
            }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(base_output_dir, f"streaming_optimized_{timestamp}")
        ensure_dir(output_dir)
        
        streaming_logger.info(f"ğŸ“ æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        streaming_logger.info(f"ğŸ“Š æ‰¾åˆ° {len(doc_paths)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        streaming_logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
        
        document_results = {}
        
        # ä½¿ç”¨LLMProcessorçš„ä¼˜åŒ–æµå¼å¤„ç†æ–¹æ³•
        try:
            streaming_logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–æµå¼å¤„ç†æ‰€æœ‰æ–‡æ¡£...")
            
            # ä½¿ç”¨processorçš„process_documents_streaming_optimizedæ–¹æ³•
            for doc_path, doc_chunk_results in self.llm_processor.process_documents_streaming_optimized(
                doc_paths, include_tables=include_tables
            ):
                try:
                    # è½¬æ¢ç»“æœæ ¼å¼ï¼Œä¿ç•™å®Œæ•´çš„processing_info
                    chunk_results = []
                    for i, (result, info) in enumerate(doc_chunk_results):
                        chunk_results.append({
                            'chunk_index': i,
                            'result': result if info['success'] else None,
                            'success': info['success'],
                            'error': info.get('error') if not info['success'] else None,
                            'processing_time': info.get('processing_time', 0),
                            # ä¿ç•™å®Œæ•´çš„processing_info
                            'full_info': info
                        })
                    
                    # å¤„ç†å•ä¸ªæ–‡æ¡£çš„ç»“æœ
                    doc_result = self._process_document_results(
                        doc_path, chunk_results, output_dir, generate_validation_report
                    )
                    
                    document_results[doc_path] = doc_result
                    
                    # è¿›åº¦æ˜¾ç¤º
                    status = "âœ…" if doc_result['success'] else "âŒ"
                    streaming_logger.info(f"{status} æ–‡æ¡£å¤„ç†å®Œæˆ: {Path(doc_path).name}")
                    
                except Exception as e:
                    streaming_logger.error(f"âŒ å¤„ç†æ–‡æ¡£ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {doc_path} - {e}")
                    document_results[doc_path] = {
                        'success': False,
                        'error': f"ç»“æœå¤„ç†å¤±è´¥: {e}",
                        'processing_time': 0
                    }
        
        except Exception as e:
            streaming_logger.error(f"âŒ ä¼˜åŒ–æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # ä¸ºæ‰€æœ‰æœªå¤„ç†çš„æ–‡æ¡£æ·»åŠ é”™è¯¯è®°å½•
            for doc_path in doc_paths:
                if doc_path not in document_results:
                    document_results[doc_path] = {
                        'success': False,
                        'error': f"æµå¼å¤„ç†å¤±è´¥: {e}",
                        'processing_time': 0
                    }
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        results = self._generate_final_results(
            document_results, output_dir, total_time, "streaming_optimized"
        )
        
        # æ˜¾ç¤ºç»“æœ
        self._display_results(results)
        
        return results
    
    def _get_document_chunks(self, doc_path: str, include_tables: bool = True) -> List[str]:
        """è·å–æ–‡æ¡£åˆ†å—"""
        file_ext = Path(doc_path).suffix.lower()
        
        if file_ext in ['.docx', '.doc']:
            # Wordæ–‡æ¡£å¤„ç†
            if include_tables:
                return self.word_chunker.chunk_document_with_tables(doc_path)
            else:
                return self.word_chunker.chunk_document(doc_path)
        else:
            # çº¯æ–‡æœ¬æ–‡ä»¶å¤„ç†
            with open(doc_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            from .utils import chunk_text
            return chunk_text(
                text,
                self.config.processing_config.chunk_size,
                self.config.processing_config.chunk_overlap
            )
    
    def _process_document_results(self, 
                                doc_path: str, 
                                chunk_results: List[Dict[str, Any]], 
                                output_dir: str,
                                generate_validation_report: bool = True) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£ç»“æœ"""
        doc_name = Path(doc_path).stem
        safe_doc_name = sanitize_filename(doc_name)
        
        # åˆ›å»ºæ–‡æ¡£è¾“å‡ºç›®å½•
        doc_output_dir = os.path.join(output_dir, safe_doc_name)
        ensure_dir(doc_output_dir)
        
        # åˆ†ç¦»æˆåŠŸå’Œå¤±è´¥çš„ç»“æœï¼Œä¿ç•™è¯¦ç»†ä¿¡æ¯
        successful_results = []
        failed_chunks = []
        detailed_chunk_info = []
        processing_stats = {
            'total_processing_time': 0,
            'total_chunk_length': 0,
            'total_response_length': 0,
            'reasoning_available_count': 0,
            'model_used': None
        }
        
        for chunk_result in chunk_results:
            chunk_index = chunk_result['chunk_index']
            full_info = chunk_result.get('full_info', {})
            
            # æ„å»ºè¯¦ç»†çš„åˆ†å—ä¿¡æ¯
            chunk_detail = {
                'chunk_index': chunk_index,
                'success': chunk_result['success'],
                'processing_time': chunk_result.get('processing_time', 0),
                'error': chunk_result.get('error') if not chunk_result['success'] else None,
                # ä»full_infoä¸­æå–è¯¦ç»†ä¿¡æ¯
                'model': full_info.get('model'),
                'chunk_length': full_info.get('chunk_length', 0),
                'response_length': full_info.get('response_length', 0),
                'has_reasoning': bool(full_info.get('reasoning', '')),
                'reasoning_length': len(full_info.get('reasoning', '')),
                'doc_name': full_info.get('doc_name'),
                'global_index': full_info.get('global_index'),
                'raw_response_preview': full_info.get('raw_response', '')[:200] if not chunk_result['success'] else None
            }
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_stats['total_processing_time'] += chunk_result.get('processing_time', 0)
            processing_stats['total_chunk_length'] += full_info.get('chunk_length', 0)
            processing_stats['total_response_length'] += full_info.get('response_length', 0)
            
            if full_info.get('reasoning'):
                processing_stats['reasoning_available_count'] += 1
            
            if not processing_stats['model_used'] and full_info.get('model'):
                processing_stats['model_used'] = full_info.get('model')
            
            if chunk_result['success'] and chunk_result['result']:
                successful_results.append(chunk_result['result'])
                
                chunk_detail.update({
                    'entity_counts': {
                        'basic_entities': len(chunk_result['result'].get('åŸºç¡€å®ä½“', [])),
                        'state_entities': len(chunk_result['result'].get('çŠ¶æ€å®ä½“', [])),
                        'relations': len(chunk_result['result'].get('çŠ¶æ€å…³ç³»', []))
                    }
                })
            else:
                failed_chunks.append({
                    'chunk_index': chunk_index,
                    'error': chunk_result.get('error', 'æœªçŸ¥é”™è¯¯'),
                    'processing_time': chunk_result.get('processing_time', 0),
                    'model': full_info.get('model'),
                    'chunk_length': full_info.get('chunk_length', 0),
                    'raw_response_preview': full_info.get('raw_response', '')[:500] if full_info.get('raw_response') else None
                })
            
            detailed_chunk_info.append(chunk_detail)
        
        # ä¿å­˜è¯¦ç»†çš„åˆ†å—ç»“æœ
        chunks_file = os.path.join(doc_output_dir, "chunks_results.json")
        save_json({
            'document_info': {
                'document_path': doc_path,
                'document_name': doc_name,
                'processing_timestamp': datetime.now().isoformat()
            },
            'processing_summary': {
                'total_chunks': len(chunk_results),
                'successful_chunks': len(successful_results),
                'failed_chunks': len(failed_chunks),
                'success_rate': (len(successful_results) / len(chunk_results) * 100) if chunk_results else 0,
                'total_processing_time': processing_stats['total_processing_time'],
                'average_processing_time': processing_stats['total_processing_time'] / len(chunk_results) if chunk_results else 0,
                'total_chunk_length': processing_stats['total_chunk_length'],
                'total_response_length': processing_stats['total_response_length'],
                'average_chunk_length': processing_stats['total_chunk_length'] / len(chunk_results) if chunk_results else 0,
                'average_response_length': processing_stats['total_response_length'] / len(successful_results) if successful_results else 0,
                'reasoning_available_count': processing_stats['reasoning_available_count'],
                'reasoning_coverage': (processing_stats['reasoning_available_count'] / len(chunk_results) * 100) if chunk_results else 0,
                'model_used': processing_stats['model_used']
            },
            'detailed_chunks': detailed_chunk_info,
            'successful_chunks_data': successful_results,
            'failed_chunks': failed_chunks
        }, chunks_file)
        
        # å¦‚æœæœ‰reasoningæ•°æ®ï¼Œå•ç‹¬ä¿å­˜
        reasoning_data = []
        for chunk_result in chunk_results:
            full_info = chunk_result.get('full_info', {})
            reasoning = full_info.get('reasoning', '')
            if reasoning:
                reasoning_data.append({
                    'chunk_index': chunk_result['chunk_index'],
                    'reasoning': reasoning,
                    'success': chunk_result['success']
                })
        
        if reasoning_data:
            reasoning_file = os.path.join(doc_output_dir, "reasoning_data.json")
            save_json({
                'document_name': doc_name,
                'reasoning_chunks': reasoning_data,
                'summary': {
                    'total_reasoning_chunks': len(reasoning_data),
                    'coverage': (len(reasoning_data) / len(chunk_results) * 100) if chunk_results else 0
                }
            }, reasoning_file)
        
        if not successful_results:
            # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆä»chunk_resultsä¸­è·å–ï¼‰
            processing_time = sum(chunk.get('processing_time', 0) for chunk in chunk_results)
            
            return {
                'success': False,
                'error': 'æ‰€æœ‰åˆ†å—å¤„ç†éƒ½å¤±è´¥äº†',
                'output_directory': doc_output_dir,
                'processing_time': processing_time,
                'chunks': {
                    'total': len(chunk_results),
                    'successful': 0,
                    'failed': len(failed_chunks),
                    'success_rate': 0
                }
            }
        
        # åˆå¹¶ç»“æœ
        merged_data = merge_knowledge_graph_results(successful_results)
        
        # å¦‚æœæœ‰å¤±è´¥çš„å—ï¼Œä¿å­˜å¤±è´¥ä¿¡æ¯
        failed_file = None
        if failed_chunks:
            failed_file = os.path.join(doc_output_dir, "failed_chunks.json")
            save_json({
                'document_info': {
                    'document_path': doc_path,
                    'document_name': doc_name,
                    'processing_timestamp': datetime.now().isoformat()
                },
                'failed_summary': {
                    'total_failed_chunks': len(failed_chunks),
                    'total_chunks': len(chunk_results)
                },
                'failed_chunks': failed_chunks
            }, failed_file)
            self.logger.warning(f"å¤±è´¥ä¿¡æ¯å·²ä¿å­˜: {failed_file}")
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        merged_file = os.path.join(doc_output_dir, "merged_data.json")
        save_json(merged_data, merged_file)
        
        # æ•°æ®éªŒè¯
        validation_result = None
        if generate_validation_report:
            try:
                validated_data, validation_report = self.validator.validate_data(merged_data)
                validation_result = self.validator.get_validation_summary()
                
                # ä¿å­˜éªŒè¯æŠ¥å‘Š
                self.validator.export_validation_report(
                    os.path.join(doc_output_dir, "validation_report.json")
                )
                
                # ä¿å­˜éªŒè¯åçš„æ•°æ®
                save_json(validated_data, os.path.join(doc_output_dir, "validated_data.json"))
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        
        # è·å–LLMç»Ÿè®¡ä¿¡æ¯
        llm_stats = self.llm_processor.get_stats()
        
        # è®¡ç®—å¤„ç†æ—¶é—´ï¼ˆä»chunk_resultsä¸­è·å–ï¼‰
        processing_time = sum(chunk.get('processing_time', 0) for chunk in chunk_results)
        
        return {
            'success': True,
            'output_directory': doc_output_dir,
            'processing_time': processing_time,
            'chunks': {
                'total': len(chunk_results),
                'successful': len(successful_results),
                'failed': len(failed_chunks),
                'success_rate': (len(successful_results) / len(chunk_results) * 100) if chunk_results else 0
            },
            'entities': {
                'total': len(merged_data['åŸºç¡€å®ä½“']) + len(merged_data['çŠ¶æ€å®ä½“']),
                'basic_entities': len(merged_data['åŸºç¡€å®ä½“']),
                'state_entities': len(merged_data['çŠ¶æ€å®ä½“'])
            },
            'relations': {
                'total': len(merged_data['çŠ¶æ€å…³ç³»'])
            },
            'processing_stats': {
                'total_processing_time': processing_stats['total_processing_time'],
                'average_processing_time': processing_stats['total_processing_time'] / len(chunk_results) if chunk_results else 0,
                'total_chunk_length': processing_stats['total_chunk_length'],
                'total_response_length': processing_stats['total_response_length'],
                'average_chunk_length': processing_stats['total_chunk_length'] / len(chunk_results) if chunk_results else 0,
                'average_response_length': processing_stats['total_response_length'] / len(successful_results) if successful_results else 0,
                'reasoning_available_count': processing_stats['reasoning_available_count'],
                'reasoning_coverage': (processing_stats['reasoning_available_count'] / len(chunk_results) * 100) if chunk_results else 0,
                'model_used': processing_stats['model_used']
            },
            'files': {
                'chunks_results': chunks_file,
                'merged_data': merged_file,
                'failed_file': failed_file,
                'reasoning_data': os.path.join(doc_output_dir, "reasoning_data.json") if reasoning_data else None,
                'validation_report': os.path.join(doc_output_dir, "validation_report.json") if validation_result else None,
                'validated_data': os.path.join(doc_output_dir, "validated_data.json") if validation_result else None
            },
            'llm_stats': llm_stats,
            'validation': validation_result
        }
    
    def _generate_final_results(self, 
                              document_results: Dict[str, Any], 
                              output_dir: str, 
                              total_time: float,
                              processing_mode: str) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»“æœ"""
        # ç”Ÿæˆå¤„ç†æ‘˜è¦
        summary = self._generate_summary(document_results, total_time)
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report = {
            'processing_info': {
                'mode': processing_mode,
                'model': self.config.llm_config.model,
                'timestamp': datetime.now().isoformat(),
                'output_directory': output_dir,
                'config': {
                    'chunk_size': self.config.processing_config.chunk_size,
                    'chunk_overlap': self.config.processing_config.chunk_overlap,
                    'parallel_processing': self.config.processing_config.enable_parallel,
                    'max_workers': self.config.processing_config.max_workers
                }
            },
            'summary': summary,
            'document_results': document_results
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = os.path.join(output_dir, "final_report.json")
        save_json(final_report, report_file)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£çš„merged_data.json
        merged_kg_file = os.path.join(output_dir, "all_documents_merged_knowledge_graph.json")
        self.merge_all_documents_knowledge_graph(
            [res['files']['merged_data'] for res in document_results.values() if res.get('files', {}).get('merged_data')],
            merged_kg_file
        )
        
        # åˆ¤æ–­æ˜¯å¦æœ‰éªŒè¯åçš„æ•°æ®éœ€è¦åˆå¹¶
        validated_files = [res['files']['validation_report'] for res in document_results.values() if res.get('files', {}).get('validation_report')]
        if validated_files:
            merged_validated_file = os.path.join(output_dir, "all_documents_validated_data.json")
            self.merge_all_documents_knowledge_graph(
                [res['files']['validated_data'] for res in document_results.values() if res.get('files', {}).get('validated_data')],
                merged_validated_file
            )

        return final_report
    
    def merge_all_documents_knowledge_graph(self, file_paths: List[str], output_path: str) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰æ–‡æ¡£çš„çŸ¥è¯†å›¾è°±ç»“æœ"""
        all_merged_entities = {
            'åŸºç¡€å®ä½“': [],
            'çŠ¶æ€å®ä½“': [],
            'çŠ¶æ€å…³ç³»': []
        }
        for file_path in file_paths:
            if os.path.exists(file_path):
                merged_data = load_json(file_path)
                all_merged_entities['åŸºç¡€å®ä½“'].extend(merged_data.get('åŸºç¡€å®ä½“', []))
                all_merged_entities['çŠ¶æ€å®ä½“'].extend(merged_data.get('çŠ¶æ€å®ä½“', []))
                all_merged_entities['çŠ¶æ€å…³ç³»'].extend(merged_data.get('çŠ¶æ€å…³ç³»', []))
        save_json(all_merged_entities, output_path)
        return all_merged_entities

    def _generate_summary(self, document_results: Dict[str, Any], total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        successful_docs = sum(1 for result in document_results.values() if result.get('success', False))
        total_docs = len(document_results)
        
        total_chunks = 0
        total_successful_chunks = 0
        total_entities = 0
        total_relations = 0
        
        for result in document_results.values():
            if result.get('success'):
                chunks_info = result.get('chunks', {})
                total_chunks += chunks_info.get('total', 0)
                total_successful_chunks += chunks_info.get('successful', 0)
                
                entities_info = result.get('entities', {})
                total_entities += entities_info.get('total', 0)
                
                relations_info = result.get('relations', {})
                total_relations += relations_info.get('total', 0)
        
        return {
            'documents': {
                'total': total_docs,
                'successful': successful_docs,
                'failed': total_docs - successful_docs,
                'success_rate': (successful_docs / total_docs * 100) if total_docs > 0 else 0
            },
            'chunks': {
                'total': total_chunks,
                'successful': total_successful_chunks,
                'failed': total_chunks - total_successful_chunks,
                'success_rate': (total_successful_chunks / total_chunks * 100) if total_chunks > 0 else 0
            },
            'knowledge_graph': {
                'total_entities': total_entities,
                'total_relations': total_relations
            },
            'performance': {
                'total_processing_time': total_time,
                'avg_time_per_document': total_time / total_docs if total_docs > 0 else 0
            }
        }
    
    def _display_results(self, results: Dict[str, Any]):
        """æ˜¾ç¤ºå¤„ç†ç»“æœ"""
        print("\n" + "=" * 60)
        print("æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        print("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        info = results['processing_info']
        print(f"\nğŸ“Š å¤„ç†ä¿¡æ¯:")
        print(f"  æ¨¡å¼: {info['mode']}")
        print(f"  æ¨¡å‹: {info['model']}")
        print(f"  å¤„ç†æ—¶é—´: {info['timestamp']}")
        print(f"  è¾“å‡ºç›®å½•: {info['output_directory']}")
        
        # é…ç½®ä¿¡æ¯
        config = info['config']
        print(f"\nâš™ï¸ é…ç½®å‚æ•°:")
        print(f"  åˆ†å—å¤§å°: {config['chunk_size']} tokens")
        print(f"  é‡å å¤§å°: {config['chunk_overlap']} tokens")
        print(f"  å¹¶è¡Œå¤„ç†: {config['parallel_processing']}")
        print(f"  æœ€å¤§çº¿ç¨‹æ•°: {config['max_workers']}")
        
        # å¤„ç†æ‘˜è¦
        summary = results['summary']
        print(f"\nğŸ“ˆ å¤„ç†æ‘˜è¦:")
        print(f"  æ–‡æ¡£æ€»æ•°: {summary['documents']['total']}")
        print(f"  æˆåŠŸæ–‡æ¡£: {summary['documents']['successful']}")
        print(f"  æ–‡æ¡£æˆåŠŸç‡: {summary['documents']['success_rate']:.1f}%")
        print(f"  åˆ†å—æ€»æ•°: {summary['chunks']['total']}")
        print(f"  æˆåŠŸåˆ†å—: {summary['chunks']['successful']}")
        print(f"  åˆ†å—æˆåŠŸç‡: {summary['chunks']['success_rate']:.1f}%")
        print(f"  æ€»å®ä½“æ•°: {summary['knowledge_graph']['total_entities']}")
        print(f"  æ€»å…³ç³»æ•°: {summary['knowledge_graph']['total_relations']}")
        print(f"  æ€»å¤„ç†æ—¶é—´: {summary['performance']['total_processing_time']:.2f}ç§’")
        
        # å„æ–‡æ¡£è¯¦æƒ…
        print(f"\nğŸ“„ å„æ–‡æ¡£å¤„ç†è¯¦æƒ…:")
        for doc_path, result in results['document_results'].items():
            doc_name = os.path.basename(doc_path)
            if result.get('success'):
                chunks = result['chunks']
                entities = result['entities']
                relations = result['relations']
                proc_stats = result.get('processing_stats', {})
                
                print(f"  âœ… {doc_name}:")
                print(f"     åˆ†å—: {chunks['successful']}/{chunks['total']} ({chunks['success_rate']:.1f}%)")
                print(f"     å®ä½“: {entities['total']} (åŸºç¡€: {entities['basic_entities']}, çŠ¶æ€: {entities['state_entities']})")
                print(f"     å…³ç³»: {relations['total']}")
                print(f"     æ—¶é—´: {result['processing_time']:.2f}ç§’")
                
                # æ˜¾ç¤ºæ–°å¢çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
                if proc_stats:
                    print(f"     å¹³å‡åˆ†å—é•¿åº¦: {proc_stats.get('average_chunk_length', 0):.0f} å­—ç¬¦")
                    print(f"     å¹³å‡å“åº”é•¿åº¦: {proc_stats.get('average_response_length', 0):.0f} å­—ç¬¦")
                    if proc_stats.get('reasoning_available_count', 0) > 0:
                        print(f"     æ¨ç†è¦†ç›–ç‡: {proc_stats.get('reasoning_coverage', 0):.1f}% ({proc_stats.get('reasoning_available_count', 0)} ä¸ªåˆ†å—)")
                    print(f"     ä½¿ç”¨æ¨¡å‹: {proc_stats.get('model_used', 'Unknown')}")
                
                print(f"     è¾“å‡º: {result['output_directory']}")
            else:
                print(f"  âŒ {doc_name}: {result.get('error', 'å¤„ç†å¤±è´¥')}")
        
        print(f"\nâœ¨ å¤„ç†å®Œæˆï¼æŸ¥çœ‹è¾“å‡ºç›®å½•è·å–è¯¦ç»†ç»“æœã€‚")
    
    @log_execution_time()
    def process_single_document(self,
                                document_path: str,
                                base_output_dir: str = "output",
                                include_tables: bool = True,
                                generate_validation_report: bool = True) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£

        Args:
            document_path: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
            include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
            generate_validation_report: æ˜¯å¦ç”ŸæˆéªŒè¯æŠ¥å‘Š
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        # åˆ›å»ºå¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—å™¨
        single_logger = create_logger_with_context({
            'mode': 'single_document',
            'document_path': document_path,
            'output_dir': base_output_dir,
            'include_tables': include_tables,
            'validation': generate_validation_report
        })
        
        start_time = time.time()
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(document_path):
            single_logger.error(f"ğŸ“„ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {document_path}")
            return {
                'success': False,
                'error': f'è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {document_path}',
                'processing_time': 0
            }

        doc_paths = [document_path]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(base_output_dir, f"streaming_optimized_{timestamp}")
        ensure_dir(output_dir)
        
        single_logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
        
        document_results = {}
        
        # ä½¿ç”¨LLMProcessorçš„ä¼˜åŒ–æµå¼å¤„ç†æ–¹æ³•
        try:
            single_logger.info("ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£...")
            
            # ä½¿ç”¨processorçš„process_documents_streaming_optimizedæ–¹æ³•
            for doc_path, doc_chunk_results in self.llm_processor.process_documents_streaming_optimized(
                doc_paths, include_tables=include_tables
            ):
                try:
                    # è½¬æ¢ç»“æœæ ¼å¼ï¼Œä¿ç•™å®Œæ•´çš„processing_info
                    chunk_results = []
                    for i, (result, info) in enumerate(doc_chunk_results):
                        chunk_results.append({
                            'chunk_index': i,
                            'result': result if info['success'] else None,
                            'success': info['success'],
                            'error': info.get('error') if not info['success'] else None,
                            'processing_time': info.get('processing_time', 0),
                            # ä¿ç•™å®Œæ•´çš„processing_info
                            'full_info': info
                        })
                    
                    # å¤„ç†å•ä¸ªæ–‡æ¡£çš„ç»“æœ
                    doc_result = self._process_document_results(
                        doc_path, chunk_results, output_dir, generate_validation_report
                    )
                    
                    document_results[doc_path] = doc_result
                    
                    # è¿›åº¦æ˜¾ç¤º
                    status = "âœ…" if doc_result['success'] else "âŒ"
                    single_logger.info(f"{status} æ–‡æ¡£å¤„ç†å®Œæˆ: {Path(doc_path).name}")
                    
                except Exception as e:
                    single_logger.error(f"âŒ å¤„ç†æ–‡æ¡£ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {doc_path} - {e}")
                    document_results[doc_path] = {
                        'success': False,
                        'error': f"ç»“æœå¤„ç†å¤±è´¥: {e}",
                        'processing_time': 0
                    }
        
        except Exception as e:
            single_logger.error(f"âŒ ä¼˜åŒ–æµå¼å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # ä¸ºæ‰€æœ‰æœªå¤„ç†çš„æ–‡æ¡£æ·»åŠ é”™è¯¯è®°å½•
            for doc_path in doc_paths:
                if doc_path not in document_results:
                    document_results[doc_path] = {
                        'success': False,
                        'error': f"æµå¼å¤„ç†å¤±è´¥: {e}",
                        'processing_time': 0
                    }
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        result = document_results[document_path]
        
        return result
        
    # def process_single_document(self,
    #                            input_path: str,
    #                            output_dir: str = "output",
    #                            doc_name: Optional[str] = None,
    #                            chunk_size: Optional[int] = None,
    #                            chunk_overlap: Optional[int] = None,
    #                            include_tables: bool = True,
    #                            verbose: bool = False) -> Dict[str, Any]:
    #     """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆé€ä¸ªå¤„ç†æ–‡æœ¬å—ï¼‰
        
    #     Args:
    #         input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    #         output_dir: è¾“å‡ºç›®å½•
    #         doc_name: æ–‡æ¡£åç§°ï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
    #         chunk_size: æ–‡æœ¬å—å¤§å°ï¼ˆè¦†ç›–é…ç½®ï¼‰
    #         chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°ï¼ˆè¦†ç›–é…ç½®ï¼‰
    #         include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
    #         verbose: è¯¦ç»†è¾“å‡º
            
    #     Returns:
    #         å¤„ç†ç»“æœå­—å…¸
    #     """
    #     start_time = time.time()
        
    #     # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    #     if not os.path.exists(input_path):
    #         return {
    #             'success': False,
    #             'error': f'è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}',
    #             'processing_time': 0
    #         }
        
    #     try:
    #         # è®¾ç½®æ—¥å¿—çº§åˆ«
    #         if verbose:
    #             self.logger.setLevel("DEBUG")
            
    #         # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    #         ensure_dir(output_dir)
            
    #         # è·å–æ–‡æ¡£åç§°
    #         if not doc_name:
    #             doc_name = Path(input_path).stem
            
    #         self.logger.info(f"å¼€å§‹å¤„ç†å•ä¸ªæ–‡æ¡£: {input_path}")
    #         self.logger.info(f"æ–‡æ¡£åç§°: {doc_name}")
    #         self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
            
    #         # ä½¿ç”¨processorçš„process_documentsæ–¹æ³•å¤„ç†å•ä¸ªæ–‡æ¡£
    #         self.logger.info("ä½¿ç”¨processorå¤„ç†å•ä¸ªæ–‡æ¡£")
            
    #         # ä¸´æ—¶æ›´æ–°åˆ†å—é…ç½®ï¼ˆå¦‚æœæä¾›äº†è¦†ç›–å‚æ•°ï¼‰
    #         original_chunk_size = None
    #         original_chunk_overlap = None
    #         if chunk_size or chunk_overlap:
    #             original_chunk_size = self.llm_processor.chunk_size
    #             original_chunk_overlap = self.llm_processor.chunk_overlap
    #             if chunk_size:
    #                 self.llm_processor.chunk_size = chunk_size
    #             if chunk_overlap:
    #                 self.llm_processor.chunk_overlap = chunk_overlap
            
    #         try:
    #             # ä½¿ç”¨processorçš„process_documentsæ–¹æ³•
    #             batch_results = self.llm_processor.process_documents(
    #                 [input_path], include_tables=include_tables
    #             )
                
    #             # è·å–å•ä¸ªæ–‡æ¡£çš„å¤„ç†ç»“æœ
    #             doc_chunk_results = batch_results.get(input_path, [])
                
    #             # è½¬æ¢ç»“æœæ ¼å¼
    #             all_results = []
    #             failed_chunks = []
                
    #             for i, (result, info) in enumerate(doc_chunk_results):
    #                 if info['success'] and result:
    #                     all_results.append(result)
    #                     self.logger.info(f"å— {i+1} å¤„ç†æˆåŠŸ")
    #                 else:
    #                     failed_chunks.append({
    #                         'chunk_index': i,
    #                         'error': info.get('error', 'æœªçŸ¥é”™è¯¯'),
    #                         'processing_time': info.get('processing_time', 0)
    #                     })
    #                     self.logger.error(f"å— {i+1} å¤„ç†å¤±è´¥: {info.get('error')}")
                
    #             self.logger.info(f"å…±å¤„ç† {len(doc_chunk_results)} ä¸ªæ–‡æœ¬å—ï¼ŒæˆåŠŸ {len(all_results)} ä¸ª")
                
    #             # åˆå¹¶ç»“æœ
    #             self.logger.info("åˆå¹¶å¤„ç†ç»“æœ...")
    #             merged_result = merge_knowledge_graph_results(all_results)
                
    #         finally:
    #             # æ¢å¤åŸå§‹é…ç½®
    #             if original_chunk_size is not None:
    #                 self.llm_processor.chunk_size = original_chunk_size
    #             if original_chunk_overlap is not None:
    #                 self.llm_processor.chunk_overlap = original_chunk_overlap
            
    #         # ä¿å­˜ç»“æœ
    #         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #         safe_doc_name = sanitize_filename(doc_name)
    #         output_file = os.path.join(output_dir, f"{safe_doc_name}_{timestamp}_result.json")
    #         save_json(merged_result, output_file)
    #         self.logger.info(f"ç»“æœå·²ä¿å­˜: {output_file}")
            
    #         # ä¿å­˜å¤±è´¥ä¿¡æ¯
    #         failed_file = None
    #         if failed_chunks:
    #             failed_file = os.path.join(output_dir, f"{safe_doc_name}_{timestamp}_failed.json")
    #             save_json(failed_chunks, failed_file)
    #             self.logger.warning(f"å¤±è´¥ä¿¡æ¯å·²ä¿å­˜: {failed_file}")
            
    #         # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    #         stats = self.llm_processor.get_stats()
    #         processing_time = time.time() - start_time
            
    #         total_chunks = len(doc_chunk_results) if 'doc_chunk_results' in locals() else 0
            
    #         report = {
    #             'document_name': doc_name,
    #             'input_path': input_path,
    #             'processing_time': processing_time,
    #             'total_chunks': total_chunks,
    #             'successful_chunks': len(all_results),
    #             'failed_chunks': len(failed_chunks),
    #             'success_rate': (len(all_results) / total_chunks) * 100 if total_chunks > 0 else 0,
    #             'llm_stats': stats,
    #             'result_summary': {
    #                 'total_entities': len(merged_result.get('åŸºç¡€å®ä½“', [])),
    #                 'total_states': len(merged_result.get('çŠ¶æ€å®ä½“', [])),
    #                 'total_relations': len(merged_result.get('çŠ¶æ€å…³ç³»', []))
    #             },
    #             'files': {
    #                 'result_file': output_file,
    #                 'failed_file': failed_file
    #             }
    #         }
            
    #         report_file = os.path.join(output_dir, f"{safe_doc_name}_{timestamp}_report.json")
    #         save_json(report, report_file)
            
    #         # è¿”å›ç»“æœ
    #         return {
    #             'success': True,
    #             'document_name': doc_name,
    #             'input_path': input_path,
    #             'output_directory': output_dir,
    #             'processing_time': processing_time,
    #             'chunks': {
    #                 'total': total_chunks,
    #                 'successful': len(all_results),
    #                 'failed': len(failed_chunks),
    #                 'success_rate': report['success_rate']
    #             },
    #             'entities': {
    #                 'total': report['result_summary']['total_entities'] + report['result_summary']['total_states'],
    #                 'basic_entities': report['result_summary']['total_entities'],
    #                 'state_entities': report['result_summary']['total_states']
    #             },
    #             'relations': {
    #                 'total': report['result_summary']['total_relations']
    #             },
    #             'files': {
    #                 'result_file': output_file,
    #                 'failed_file': failed_file,
    #                 'report_file': report_file
    #             },
    #             'llm_stats': stats
    #         }
            
    #     except Exception as e:
    #         self.logger.error(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    #         return {
    #             'success': False,
    #             'error': str(e),
    #             'processing_time': time.time() - start_time
    #         }