#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM JSON Generator å‘½ä»¤è¡Œæ¥å£

æä¾›å‘½ä»¤è¡Œå·¥å…·æ¥ä½¿ç”¨LLM JSON GeneratoråŒ…çš„åŠŸèƒ½ã€‚
"""

import argparse
import sys
import os
from pathlib import Path
from typing import List, Optional
from datetime import datetime

from .processor import LLMProcessor
from .config import ConfigManager
from .utils import (
    ensure_dir, 
    save_json, 
    load_json,
    chunk_text,
    Timer,
    sanitize_filename,
    merge_knowledge_graph_results
)
from .log import (
    setup_logging,
    get_logger,
    create_logger_with_context,
    create_timed_logger,
    log_execution_time,
    log_system_info
)
from .exceptions import LLMProcessingError, ValidationError
from .word_chunker import WordChunker
from .run_mode import DocumentProcessor


@log_execution_time()
def create_config_command(args):
    """åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶"""
    logger = create_logger_with_context({
        'command': 'create_config',
        'output': args.output or 'config.json'
    })
    
    logger.info("ğŸ”§ å¼€å§‹åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    
    config = ConfigManager()
    
    # è®¾ç½®é»˜è®¤LLMé…ç½®
    config.llm_config.api_key = "your-openai-api-key-here"
    config.llm_config.model = "gpt-4o-mini"
    config.llm_config.temperature = 0.1
    config.llm_config.max_tokens = 4000
    config.llm_config.max_retries = 3
    config.llm_config.timeout = 60
    
    # è®¾ç½®é»˜è®¤å¤„ç†é…ç½®
    config.processing_config.chunk_size = 2000
    config.processing_config.chunk_overlap = 200
    config.processing_config.max_workers = 4
    config.processing_config.enable_parallel = True
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_file = args.output or "config.json"
    config.save_to_file(config_file)
    
    logger.info(f"âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    print(f"âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    print("âš ï¸  è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œè®¾ç½®ä½ çš„OpenAI APIå¯†é’¥")
    print(f"ğŸ“ ç¼–è¾‘å‘½ä»¤: notepad {config_file}" if os.name == 'nt' else f"ğŸ“ ç¼–è¾‘å‘½ä»¤: nano {config_file}")


@log_execution_time()
def process_text_command(args):
    """å¤„ç†å•ä¸ªæ–‡æœ¬æ–‡ä»¶"""
    # åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—å™¨
    logger = create_logger_with_context({
        'command': 'process_text',
        'document_path': args.document_path,
        'output': args.output or 'output',
        'config': args.config or 'config.json'
    })
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.document_path):
        error_msg = f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.document_path}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        return 1
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_file = args.config or "config.json"
    if not os.path.exists(config_file):
        error_msg = f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        print("ğŸ’¡ ä½¿ç”¨ 'llmjson create-config' åˆ›å»ºé…ç½®æ–‡ä»¶")
        return 1
    
    try:
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        log_level = "DEBUG" if args.log else "INFO"
        main_logger = setup_logging(log_level)
        
        # è®°å½•ç³»ç»Ÿä¿¡æ¯
        if args.log:
            log_system_info()
        
        logger.info("ğŸ¤– åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨...")
        # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
        print("ğŸ¤– åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨...")
        processor = DocumentProcessor(config_file, args.template)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = args.output or "output"
        
        # å¤„ç†å•ä¸ªæ–‡æ¡£
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {args.document_path}")
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {args.document_path}")

        result = processor.process_single_document(
                document_path=args.document_path,
                base_output_dir=output_dir,
                include_tables=args.tables,
                generate_validation_report=args.validation
            )
        
        if result['success']:
            # è®°å½•å¤„ç†ç»“æœ
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {result['processing_time']:.2f}ç§’")
            logger.info(f"ğŸ“¦ å¤„ç†ç»Ÿè®¡ - æ€»å—æ•°: {result['chunks']['total']}, "
                       f"æˆåŠŸ: {result['chunks']['successful']}, "
                       f"å¤±è´¥: {result['chunks']['failed']}")
            
            # è¾“å‡ºæ‘˜è¦
            print("\n" + "="*50)
            print("ğŸ“Š å¤„ç†å®Œæˆæ‘˜è¦")
            print("="*50)
            print(f"â±ï¸  å¤„ç†è€—æ—¶: {result['processing_time']:.2f}ç§’")
            print(f"ğŸ“¦ æ–‡æœ¬å—æ•°: {result['chunks']['total']}")
            print(f"âœ… æˆåŠŸå¤„ç†: {result['chunks']['successful']}")
            print(f"âŒ å¤„ç†å¤±è´¥: {result['chunks']['failed']}")
            print(f"ğŸ“ˆ æˆåŠŸç‡: {result['chunks']['success_rate']:.1f}%")
            print(f"ğŸ·ï¸  æå–å®ä½“: {result['entities']['total']}ä¸ª (åŸºç¡€: {result['entities']['basic_entities']}, çŠ¶æ€: {result['entities']['state_entities']})")
            print(f"ğŸ”— æå–å…³ç³»: {result['relations']['total']}ä¸ª")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {result['output_directory']}")
            
            if result['chunks']['failed'] > 0:
                warning_msg = f"æœ‰ {result['chunks']['failed']} ä¸ªæ–‡æœ¬å—å¤„ç†å¤±è´¥"
                logger.warning(f"âš ï¸ {warning_msg}")
                print(f"\nâš ï¸  {warning_msg}ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹å¤±è´¥æŠ¥å‘Š")
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤±è´¥æ–‡ä»¶è®°å½•
                if result.get('files', {}).get('failed_file'):
                    print(f"ğŸ“„ å¤±è´¥æŠ¥å‘Š: {result['files']['failed_file']}")
                elif result.get('files', {}).get('chunks_results'):
                    print(f"ğŸ“„ è¯¦ç»†ç»“æœ: {result['files']['chunks_results']}")
                        
            return 0
        else:
            error_msg = f"å¤„ç†å¤±è´¥: {result['error']}"
            logger.error(f"âŒ {error_msg}")
            print(f"âŒ {error_msg}")
            return 1
        
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        if args.log:
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            traceback.print_exc()
        return 1


@log_execution_time()
def process_documents_command(args):
    """å¤„ç†æ–‡æ¡£æ–‡ä»¶å¤¹"""
    # åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—å™¨
    logger = create_logger_with_context({
        'command': 'process_documents',
        'folder_path': args.folder_path,
        'mode': args.mode or 'optimized',
        'output': args.output or 'output',
        'config': args.config or 'config.json'
    })
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_file = args.config or "config.json"
    if not os.path.exists(config_file):
        error_msg = f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        print("ğŸ’¡ ä½¿ç”¨ 'llmjson create-config' åˆ›å»ºé…ç½®æ–‡ä»¶")
        return 1
    
    # è·å–æ–‡ä»¶å¤¹è·¯å¾„
    folder_path = args.folder_path
    
    # éªŒè¯æ–‡ä»¶å¤¹å­˜åœ¨æ€§
    if not os.path.exists(folder_path):
        error_msg = f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        return 1
    
    if not os.path.isdir(folder_path):
        error_msg = f"æŒ‡å®šçš„è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        return 1
    
    logger.info(f"ğŸ“ å‡†å¤‡å¤„ç†æ–‡ä»¶å¤¹: {folder_path}")
    print(f"ğŸ“ å‡†å¤‡å¤„ç†æ–‡ä»¶å¤¹: {folder_path}")
    
    try:
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        log_level = "DEBUG" if args.log else "INFO"
        main_logger = setup_logging(log_level)
        
        # è®°å½•ç³»ç»Ÿä¿¡æ¯
        if args.log:
            log_system_info()
        
        logger.info("ğŸ¤– åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨...")
        # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
        print("ğŸ¤– åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨...")
        processor = DocumentProcessor(config_file, args.template)
        
        # ç¡®å®šå¤„ç†æ¨¡å¼
        mode = args.mode or "optimized"
        if mode not in ["batch", "optimized"]:
            error_msg = f"ä¸æ”¯æŒçš„å¤„ç†æ¨¡å¼: {mode}"
            logger.error(f"âŒ {error_msg}")
            print(f"âŒ {error_msg}")
            print("ğŸ’¡ æ”¯æŒçš„æ¨¡å¼: batch , optimized ")
            return 1
        
        mode_names = {
            "batch": "ä¼ ç»Ÿæ‰¹é‡å¤„ç†",
            "optimized": "ä¼˜åŒ–æµå¼å¤„ç†"
        }
        
        logger.info(f"âœ… å·²é€‰æ‹©å¤„ç†æ¨¡å¼: {mode_names[mode]}")
        print(f"\nâœ… å·²é€‰æ‹©: {mode_names[mode]}")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = args.output or "output"
        
        # å¼€å§‹å¤„ç†
        logger.info(f"ğŸ”„ å¼€å§‹{mode_names[mode]}...")
        print(f"\nğŸ”„ å¼€å§‹{mode_names[mode]}...")
        
        # ä½¿ç”¨è®¡æ—¶ä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        timed_logger = create_timed_logger({
            'operation': f'{mode}_processing',
            'folder': folder_path
        })
        
        with timed_logger.time_context("document_processing"):
            if mode == "batch":
                results = processor.process_document_list_batch(
                    folder_path=folder_path,
                    base_output_dir=output_dir,
                    include_tables=args.tables,
                    generate_validation_report=args.validation
                )
            else:  # optimized
                results = processor.process_document_list_streaming_optimized(
                    folder_path=folder_path,
                    base_output_dir=output_dir,
                    include_tables=args.tables,
                    generate_validation_report=args.validation
                )
        
        # è®°å½•å¤„ç†ç»“æœ
        success_rate = results['summary']['documents']['success_rate']
        logger.info(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {results['processing_info']['output_directory']}")
        
        print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {results['processing_info']['output_directory']}")
        
        # è¿”å›æˆåŠŸç‡ä½œä¸ºé€€å‡ºç 
        return 0 if success_rate > 80 else 1
        
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        if args.log:
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            traceback.print_exc()
        return 1


@log_execution_time()
def validate_command(args):
    """éªŒè¯JSONæ•°æ®"""
    from .validator import DataValidator
    
    # åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—å™¨
    logger = create_logger_with_context({
        'command': 'validate',
        'input': args.input,
        'output': args.output,
        'report': args.report
    })
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        error_msg = f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        return 1
    
    try:
        # åŠ è½½æ•°æ®
        logger.info(f"ğŸ“– å¼€å§‹åŠ è½½æ•°æ®: {args.input}")
        print(f"ğŸ“– åŠ è½½æ•°æ®: {args.input}")
        data = load_json(args.input)
        logger.info(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œè®°å½•æ•°: {len(data) if isinstance(data, list) else 1}")
        
        # åˆ›å»ºéªŒè¯å™¨
        logger.info("ğŸ” åˆ›å»ºæ•°æ®éªŒè¯å™¨...")
        validator = DataValidator()
        
        # éªŒè¯æ•°æ®
        logger.info("ğŸ” å¼€å§‹éªŒè¯æ•°æ®...")
        print("ğŸ” éªŒè¯æ•°æ®...")
        
        # ä½¿ç”¨è®¡æ—¶ä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        timed_logger = create_timed_logger({'operation': 'data_validation'})
        
        with timed_logger.time_context("validation"):
            validated_data, validation_report = validator.validate_data(data)
        
        # è·å–éªŒè¯æŠ¥å‘Š
        summary = validator.get_validation_summary()
        full_report = validator.get_validation_report()
        
        # è®°å½•éªŒè¯ç»“æœ
        logger.info(f"âœ… éªŒè¯å®Œæˆ - æˆåŠŸç‡: {summary['success_rate']:.1f}%, "
                   f"é”™è¯¯: {summary['error_count']}, "
                   f"è­¦å‘Š: {summary['warning_count']}, "
                   f"ä¿®å¤: {summary['correction_count']}")
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*40)
        print("ğŸ“Š éªŒè¯ç»“æœ")
        print("="*40)
        print(f"âœ… éªŒè¯æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print(f"âŒ é”™è¯¯æ•°é‡: {summary['error_count']}")
        print(f"âš ï¸  è­¦å‘Šæ•°é‡: {summary['warning_count']}")
        print(f"ğŸ”§ ä¿®å¤æ•°é‡: {summary['correction_count']}")
        
        if full_report.get('errors_deleted'):
            logger.info(f"âŒ å‘ç° {len(full_report['errors_deleted'])} ä¸ªé”™è¯¯")
            print("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for error in full_report['errors_deleted'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"  - {error}")
            if len(full_report['errors_deleted']) > 5:
                print(f"  ... è¿˜æœ‰ {len(full_report['errors_deleted']) - 5} ä¸ªé”™è¯¯")
        
        # ä¿å­˜éªŒè¯åçš„æ•°æ®
        if args.output:
            logger.info(f"ğŸ’¾ ä¿å­˜éªŒè¯åçš„æ•°æ®: {args.output}")
            save_json(validated_data, args.output)
            print(f"\nğŸ’¾ éªŒè¯åçš„æ•°æ®å·²ä¿å­˜: {args.output}")
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        if args.report:
            logger.info(f"ğŸ“„ å¯¼å‡ºéªŒè¯æŠ¥å‘Š: {args.report}")
            validator.export_validation_report(args.report)
            print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {args.report}")
        
        return 0 if summary['success_rate'] > 0.8 else 1
        
    except Exception as e:
        error_msg = f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}"
        logger.error(f"âŒ {error_msg}")
        print(f"âŒ {error_msg}")
        return 1


# merge_resultså‡½æ•°å·²è¢«merge_knowledge_graph_resultsæ›¿ä»£


def check_help_request():
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºå¸®åŠ©è¯·æ±‚
    Check if this is a help request
    """
    help_flags = ['-h', '--help']

    # æ£€æŸ¥å‚æ•°ä¸­æ˜¯å¦åŒ…å«å¸®åŠ©æ ‡å¿—
    for arg in sys.argv[1:]:
        if arg in help_flags:
            return True

        # æ£€æŸ¥æ˜¯å¦æ˜¯å­å‘½ä»¤çš„å¸®åŠ©è¯·æ±‚
        if arg in ['create-config', 'process', 'process-documents', 'validate']:
            # æ£€æŸ¥ä¸‹ä¸€ä¸ªå‚æ•°æ˜¯å¦æ˜¯å¸®åŠ©æ ‡å¿—
            arg_index = sys.argv.index(arg)
            if arg_index + 1 < len(sys.argv) and sys.argv[arg_index + 1] in help_flags:
                return True

    # æ²¡æœ‰å‚æ•°ä¹Ÿæ˜¾ç¤ºå¸®åŠ©
    return len(sys.argv) == 1


def conditional_log(logger, level, message):
    """
    æ¡ä»¶æ—¥å¿—è®°å½•å‡½æ•°
    Conditional logging function
    """
    if logger is not None:
        if level == 'info':
            logger.info(message)
        elif level == 'error':
            logger.error(message)
        elif level == 'warning':
            logger.warning(message)


def main():
    """ä¸»å‡½æ•° | Main function"""

    # é¢„è§£æå‚æ•°ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å¸®åŠ©è¯·æ±‚ï¼Œé¿å…ç”Ÿæˆæ—¥å¿—æ–‡ä»¶
    is_help_request = check_help_request()

    # åªæœ‰åœ¨éå¸®åŠ©è¯·æ±‚æ—¶æ‰è¿›è¡Œæ—¥å¿—åˆå§‹åŒ–
    main_logger = None
    if not is_help_request:
        main_logger = get_logger()
        main_logger.info("ğŸš€ LLM JSON Generator CLI å¯åŠ¨")

    # Professional bilingual help information
    description = """
LLM JSON Generator - Extract structured knowledge graphs from documents using LLMs
åŸºäºå¤§è¯­è¨€æ¨¡å‹ä»æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–çŸ¥è¯†å›¾è°±

FEATURES:
  â€¢ Document Processing    Process .txt and .docx files with intelligent chunking
  â€¢ Batch Operations       Efficient parallel processing of multiple documents
  â€¢ Data Validation        Automatic JSON validation, repair, and quality assurance
  â€¢ Flexible Configuration Environment variables, config files, and CLI options
  â€¢ Production Ready       Retry logic, error handling, and comprehensive logging

ä¸»è¦åŠŸèƒ½ï¼š
  â€¢ æ–‡æ¡£å¤„ç†    æ”¯æŒ .txt å’Œ .docx æ ¼å¼ï¼Œæ™ºèƒ½åˆ†å—å¤„ç†
  â€¢ æ‰¹é‡æ“ä½œ    å¤šæ–‡æ¡£å¹¶è¡Œå¤„ç†ï¼Œé«˜æ•ˆå¤„ç†å¤§è§„æ¨¡ä»»åŠ¡
  â€¢ æ•°æ®éªŒè¯    è‡ªåŠ¨ JSON éªŒè¯ã€ä¿®å¤å’Œè´¨é‡ä¿è¯
  â€¢ çµæ´»é…ç½®    æ”¯æŒç¯å¢ƒå˜é‡ã€é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
  â€¢ ç”Ÿäº§å°±ç»ª    é‡è¯•é€»è¾‘ã€é”™è¯¯å¤„ç†å’Œå®Œæ•´æ—¥å¿—è®°å½•
"""

    examples = """
EXAMPLES:

  Configuration Setup:
    $ llmjson create-config                    # Create default config.json
    $ llmjson create-config -o my_config.json  # Custom config path

  Single Document Processing:
    $ llmjson process document.txt             # Process with default config
    $ llmjson process report.docx -o output/   # Custom output directory
    $ llmjson process doc.txt -c config.json   # Specify config file
    $ llmjson process data.docx --tables       # Include table extraction
    $ llmjson process file.txt --validation -l # Enable validation and logging

  Batch Document Processing:
    $ llmjson process-documents ./docs/                    # Process all documents
    $ llmjson process-documents ./docs/ -m optimized       # Streaming mode (recommended)
    $ llmjson process-documents ./docs/ -m batch           # Traditional batch mode
    $ llmjson process-documents ./docs/ -c config.json \\
        --tables --validation -o results/                  # Full options

  Data Validation:
    $ llmjson validate data.json                           # Basic validation
    $ llmjson validate data.json -o clean.json             # Save cleaned data
    $ llmjson validate data.json -r report.json            # Generate report
    $ llmjson validate data.json -o clean.json -r report.json  # Both outputs

  Advanced Usage:
    $ llmjson process doc.txt -t custom_template.json      # Custom prompt template
    $ llmjson process doc.txt -c prod_config.json          # Production configuration
    $ export OPENAI_API_KEY="sk-..."                       # Set API key via env
    $ llmjson process doc.txt -l 2>&1 | tee process.log    # Capture detailed logs

OUTPUT STRUCTURE:
  output/
  â”œâ”€â”€ document_name/
  â”‚   â”œâ”€â”€ knowledge_graph.json       Final extracted knowledge graph
  â”‚   â”œâ”€â”€ chunks_results.json        Per-chunk processing results
  â”‚   â”œâ”€â”€ failed_chunks.json         Failed chunks (if any)
  â”‚   â””â”€â”€ validation_report.json     Data quality report (if --validation)

WORKFLOW:
  1. Create configuration file with API credentials
  2. Process documents to extract knowledge graph
  3. Validate and clean extracted data
  4. Use validated JSON for downstream applications

TIPS:
  â€¢ Store API key in environment variable for security: OPENAI_API_KEY
  â€¢ Use 'optimized' mode for large document batches (lower memory usage)
  â€¢ Enable --validation to ensure data quality
  â€¢ Use -l flag when troubleshooting issues

ç¤ºä¾‹ç”¨æ³•ï¼š

  é…ç½®è®¾ç½®ï¼š
    $ llmjson create-config                    # åˆ›å»ºé»˜è®¤ config.json
    $ llmjson create-config -o my_config.json  # è‡ªå®šä¹‰é…ç½®è·¯å¾„

  å•æ–‡æ¡£å¤„ç†ï¼š
    $ llmjson process document.txt             # ä½¿ç”¨é»˜è®¤é…ç½®å¤„ç†
    $ llmjson process report.docx -o output/   # æŒ‡å®šè¾“å‡ºç›®å½•
    $ llmjson process doc.txt -c config.json   # æŒ‡å®šé…ç½®æ–‡ä»¶
    $ llmjson process data.docx --tables       # æå–è¡¨æ ¼æ•°æ®
    $ llmjson process file.txt --validation -l # å¯ç”¨éªŒè¯å’Œæ—¥å¿—

  æ‰¹é‡æ–‡æ¡£å¤„ç†ï¼š
    $ llmjson process-documents ./docs/                    # å¤„ç†æ‰€æœ‰æ–‡æ¡£
    $ llmjson process-documents ./docs/ -m optimized       # æµå¼æ¨¡å¼ï¼ˆæ¨èï¼‰
    $ llmjson process-documents ./docs/ -m batch           # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼
    $ llmjson process-documents ./docs/ -c config.json \\
        --tables --validation -o results/                  # å®Œæ•´é€‰é¡¹

  æ•°æ®éªŒè¯ï¼š
    $ llmjson validate data.json                           # åŸºç¡€éªŒè¯
    $ llmjson validate data.json -o clean.json             # ä¿å­˜æ¸…ç†æ•°æ®
    $ llmjson validate data.json -r report.json            # ç”ŸæˆæŠ¥å‘Š
    $ llmjson validate data.json -o clean.json -r report.json  # åŒæ—¶è¾“å‡º

  é«˜çº§ç”¨æ³•ï¼š
    $ llmjson process doc.txt -t custom_template.json      # è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
    $ llmjson process doc.txt -c prod_config.json          # ç”Ÿäº§ç¯å¢ƒé…ç½®
    $ export OPENAI_API_KEY="sk-..."                       # é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®å¯†é’¥
    $ llmjson process doc.txt -l 2>&1 | tee process.log    # æ•è·è¯¦ç»†æ—¥å¿—

è¾“å‡ºç»“æ„ï¼š
  output/
  â”œâ”€â”€ document_name/
  â”‚   â”œâ”€â”€ knowledge_graph.json       æœ€ç»ˆæå–çš„çŸ¥è¯†å›¾è°±
  â”‚   â”œâ”€â”€ chunks_results.json        æ¯ä¸ªæ–‡æœ¬å—çš„å¤„ç†ç»“æœ
  â”‚   â”œâ”€â”€ failed_chunks.json         å¤±è´¥çš„æ–‡æœ¬å—ï¼ˆå¦‚æœ‰ï¼‰
  â”‚   â””â”€â”€ validation_report.json     æ•°æ®è´¨é‡æŠ¥å‘Šï¼ˆå¦‚å¯ç”¨ --validationï¼‰

å·¥ä½œæµç¨‹ï¼š
  1. åˆ›å»ºåŒ…å« API å‡­è¯çš„é…ç½®æ–‡ä»¶
  2. å¤„ç†æ–‡æ¡£ä»¥æå–çŸ¥è¯†å›¾è°±
  3. éªŒè¯å’Œæ¸…ç†æå–çš„æ•°æ®
  4. å°†éªŒè¯åçš„ JSON ç”¨äºä¸‹æ¸¸åº”ç”¨

ä½¿ç”¨æŠ€å·§ï¼š
  â€¢ å°† API å¯†é’¥å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­ä»¥æé«˜å®‰å…¨æ€§ï¼šOPENAI_API_KEY
  â€¢ å¯¹å¤§å‹æ–‡æ¡£æ‰¹æ¬¡ä½¿ç”¨ 'optimized' æ¨¡å¼ï¼ˆé™ä½å†…å­˜ä½¿ç”¨ï¼‰
  â€¢ å¯ç”¨ --validation ä»¥ç¡®ä¿æ•°æ®è´¨é‡
  â€¢ æ’æŸ¥é—®é¢˜æ—¶ä½¿ç”¨ -l æ ‡å¿—
"""

    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=examples
    )
    
    subparsers = parser.add_subparsers(
        dest='command',
        title='COMMANDS',
        description='Available commands for document processing and validation',
        metavar='{create-config,process,process-documents,validate}',
        help='command to execute (use "llmjson <command> -h" for details)'
    )

    # Create configuration command
    config_help = """
Generate a configuration file with default LLM and processing settings.

This command creates a JSON configuration file containing:
  â€¢ LLM settings (API key, model, temperature, tokens, retry logic)
  â€¢ Processing settings (chunk size, overlap, parallel workers)

The generated file can be customized and used with -c/--config option.

USAGE:
  llmjson create-config [-o OUTPUT]

EXAMPLES:
  $ llmjson create-config                    # Creates config.json in current directory
  $ llmjson create-config -o app/config.json # Custom output path

NOTE: Remember to edit the file and set your actual API key before use.

ç”ŸæˆåŒ…å«é»˜è®¤ LLM å’Œå¤„ç†è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚

æ­¤å‘½ä»¤åˆ›å»ºåŒ…å«ä»¥ä¸‹å†…å®¹çš„ JSON é…ç½®æ–‡ä»¶ï¼š
  â€¢ LLM è®¾ç½®ï¼ˆAPI å¯†é’¥ã€æ¨¡å‹ã€æ¸©åº¦ã€ä»¤ç‰Œæ•°ã€é‡è¯•é€»è¾‘ï¼‰
  â€¢ å¤„ç†è®¾ç½®ï¼ˆåˆ†å—å¤§å°ã€é‡å ã€å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼‰

ç”Ÿæˆçš„æ–‡ä»¶å¯ä»¥è‡ªå®šä¹‰å¹¶é€šè¿‡ -c/--config é€‰é¡¹ä½¿ç”¨ã€‚

ç”¨æ³•ï¼š
  llmjson create-config [-o è¾“å‡ºè·¯å¾„]

ç¤ºä¾‹ï¼š
  $ llmjson create-config                    # åœ¨å½“å‰ç›®å½•åˆ›å»º config.json
  $ llmjson create-config -o app/config.json # è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„

æ³¨æ„ï¼šè¯·è®°å¾—ç¼–è¾‘æ–‡ä»¶å¹¶è®¾ç½®å®é™…çš„ API å¯†é’¥åå†ä½¿ç”¨ã€‚
"""
    config_parser = subparsers.add_parser('create-config',
                                         help='Generate configuration file with default settings',
                                         description=config_help,
                                         formatter_class=argparse.RawDescriptionHelpFormatter)
    config_parser.add_argument('-o', '--output',
                             metavar='FILE',
                             help='output path for configuration file (default: config.json)')
    config_parser.set_defaults(func=create_config_command)

    # Process document command
    process_help = """
Extract entities and relationships from a document to build a knowledge graph.

Processes a single document (.txt or .docx), automatically chunks the text, sends
each chunk to the LLM for entity/relationship extraction, and aggregates results
into a unified knowledge graph.

SUPPORTED FORMATS:
  â€¢ Plain text files (.txt)
  â€¢ Microsoft Word documents (.docx)

OUTPUT FILES:
  knowledge_graph.json       Final aggregated knowledge graph
  chunks_results.json        Detailed per-chunk processing results
  failed_chunks.json         Information about failed chunks (if any)
  validation_report.json     Data quality report (with --validation flag)

USAGE:
  llmjson process DOCUMENT [OPTIONS]

OPTIONS:
  -c, --config FILE         Configuration file path (default: config.json)
  -o, --output DIR          Output directory (default: output)
  -t, --template FILE       Custom prompt template file
  --tables                  Extract and process table content
  --validation              Validate and clean extracted data
  -l, --log                 Enable detailed console logging

EXAMPLES:
  $ llmjson process document.txt                      # Basic processing
  $ llmjson process report.docx -o results/           # Custom output directory
  $ llmjson process data.txt -c custom_config.json    # Custom configuration
  $ llmjson process tables.docx --tables --validation # Extract tables with validation
  $ llmjson process debug.txt -l                      # Enable detailed logging

å¤„ç†å•ä¸ªæ–‡æ¡£ä»¥æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆ.txt æˆ– .docxï¼‰ï¼Œè‡ªåŠ¨åˆ†å—æ–‡æœ¬ï¼Œå°†æ¯ä¸ªå—å‘é€åˆ° LLM è¿›è¡Œ
å®ä½“/å…³ç³»æå–ï¼Œå¹¶å°†ç»“æœèšåˆä¸ºç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±ã€‚

æ”¯æŒæ ¼å¼ï¼š
  â€¢ çº¯æ–‡æœ¬æ–‡ä»¶ (.txt)
  â€¢ Microsoft Word æ–‡æ¡£ (.docx)

è¾“å‡ºæ–‡ä»¶ï¼š
  knowledge_graph.json       æœ€ç»ˆèšåˆçš„çŸ¥è¯†å›¾è°±
  chunks_results.json        è¯¦ç»†çš„æ¯å—å¤„ç†ç»“æœ
  failed_chunks.json         å¤±è´¥å—çš„ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
  validation_report.json     æ•°æ®è´¨é‡æŠ¥å‘Šï¼ˆä½¿ç”¨ --validation æ ‡å¿—ï¼‰

ç”¨æ³•ï¼š
  llmjson process æ–‡æ¡£ [é€‰é¡¹]

é€‰é¡¹ï¼š
  -c, --config FILE         é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šconfig.jsonï¼‰
  -o, --output DIR          è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šoutputï¼‰
  -t, --template FILE       è‡ªå®šä¹‰æç¤ºæ¨¡æ¿æ–‡ä»¶
  --tables                  æå–å’Œå¤„ç†è¡¨æ ¼å†…å®¹
  --validation              éªŒè¯å’Œæ¸…ç†æå–çš„æ•°æ®
  -l, --log                 å¯ç”¨è¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—

ç¤ºä¾‹ï¼š
  $ llmjson process document.txt                      # åŸºç¡€å¤„ç†
  $ llmjson process report.docx -o results/           # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  $ llmjson process data.txt -c custom_config.json    # è‡ªå®šä¹‰é…ç½®
  $ llmjson process tables.docx --tables --validation # æå–è¡¨æ ¼å¹¶éªŒè¯
  $ llmjson process debug.txt -l                      # å¯ç”¨è¯¦ç»†æ—¥å¿—
"""
    process_parser = subparsers.add_parser('process',
                                         help='Process a single document to extract knowledge graph',
                                         description=process_help,
                                         formatter_class=argparse.RawDescriptionHelpFormatter)
    process_parser.add_argument('document_path',
                             metavar='DOCUMENT',
                             help='path to document file (.txt or .docx)')
    process_parser.add_argument('-c', '--config',
                             metavar='FILE',
                             help='configuration file path (default: config.json)')
    process_parser.add_argument('-o', '--output',
                             metavar='DIR',
                             help='output directory (default: output)')
    process_parser.add_argument('-t', '--template',
                             metavar='FILE',
                             help='custom prompt template file')
    process_parser.add_argument('--tables', action='store_true',
                             help='extract and process tables from documents')
    process_parser.add_argument('--validation', action='store_true',
                             help='validate and clean extracted data')
    process_parser.add_argument('-l', '--log', action='store_true',
                             help='enable detailed console logging')
    process_parser.set_defaults(func=process_text_command)

    # Process documents batch command
    docs_help = """
Batch process all documents in a folder with configurable processing modes.

Recursively discovers and processes all .txt and .docx files in the specified
folder. Supports two processing modes optimized for different scenarios.

PROCESSING MODES:
  batch                     Load all documents into memory, process in parallel
                            Best for: Small to medium datasets, ample RAM
  
  optimized (recommended)   Stream documents with batched processing
                            Best for: Large datasets, memory constraints
                            Lower memory footprint, better scalability

OUTPUT STRUCTURE:
  results/
  â”œâ”€â”€ document1/
  â”‚   â”œâ”€â”€ knowledge_graph.json      Extracted knowledge graph
  â”‚   â”œâ”€â”€ chunks_results.json       Per-chunk details
  â”‚   â””â”€â”€ validation_report.json    Quality report (if --validation)
  â”œâ”€â”€ document2/
  â”‚   â””â”€â”€ ...
  â””â”€â”€ processing_summary.json       Overall statistics

USAGE:
  llmjson process-documents FOLDER [OPTIONS]

OPTIONS:
  -c, --config FILE         Configuration file path (default: config.json)
  -o, --output DIR          Output directory (default: output)
  -m, --mode MODE           Processing mode: batch or optimized (default: optimized)
  -t, --template FILE       Custom prompt template file
  --tables                  Extract and process table content
  --validation              Validate and clean extracted data
  -l, --log                 Enable detailed console logging

EXAMPLES:
  $ llmjson process-documents ./documents/                    # Process with defaults
  $ llmjson process-documents ./docs/ -m batch                # Traditional batch mode
  $ llmjson process-documents ./docs/ -m optimized -o out/    # Streaming mode
  $ llmjson process-documents ./docs/ --tables --validation   # Extract tables with validation
  $ llmjson process-documents ./docs/ -c prod_config.json -l  # Production config with logging

æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£ï¼Œæ”¯æŒå¯é…ç½®çš„å¤„ç†æ¨¡å¼ã€‚

é€’å½’å‘ç°å¹¶å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ .txt å’Œ .docx æ–‡ä»¶ã€‚æ”¯æŒé’ˆå¯¹ä¸åŒåœºæ™¯
ä¼˜åŒ–çš„ä¸¤ç§å¤„ç†æ¨¡å¼ã€‚

å¤„ç†æ¨¡å¼ï¼š
  batch                     å°†æ‰€æœ‰æ–‡æ¡£åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¹¶è¡Œå¤„ç†
                            æœ€é€‚åˆï¼šä¸­å°å‹æ•°æ®é›†ï¼Œå……è¶³çš„ RAM
  
  optimizedï¼ˆæ¨èï¼‰         æµå¼å¤„ç†æ–‡æ¡£ï¼Œåˆ†æ‰¹å¤„ç†
                            æœ€é€‚åˆï¼šå¤§å‹æ•°æ®é›†ï¼Œå†…å­˜å—é™
                            æ›´ä½çš„å†…å­˜å ç”¨ï¼Œæ›´å¥½çš„å¯æ‰©å±•æ€§

è¾“å‡ºç»“æ„ï¼š
  results/
  â”œâ”€â”€ document1/
  â”‚   â”œâ”€â”€ knowledge_graph.json      æå–çš„çŸ¥è¯†å›¾è°±
  â”‚   â”œâ”€â”€ chunks_results.json       æ¯å—è¯¦ç»†ä¿¡æ¯
  â”‚   â””â”€â”€ validation_report.json    è´¨é‡æŠ¥å‘Šï¼ˆå¦‚ --validationï¼‰
  â”œâ”€â”€ document2/
  â”‚   â””â”€â”€ ...
  â””â”€â”€ processing_summary.json       æ€»ä½“ç»Ÿè®¡

ç”¨æ³•ï¼š
  llmjson process-documents æ–‡ä»¶å¤¹ [é€‰é¡¹]

é€‰é¡¹ï¼š
  -c, --config FILE         é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šconfig.jsonï¼‰
  -o, --output DIR          è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šoutputï¼‰
  -m, --mode MODE           å¤„ç†æ¨¡å¼ï¼šbatch æˆ– optimizedï¼ˆé»˜è®¤ï¼šoptimizedï¼‰
  -t, --template FILE       è‡ªå®šä¹‰æç¤ºæ¨¡æ¿æ–‡ä»¶
  --tables                  æå–å’Œå¤„ç†è¡¨æ ¼å†…å®¹
  --validation              éªŒè¯å’Œæ¸…ç†æå–çš„æ•°æ®
  -l, --log                 å¯ç”¨è¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—

ç¤ºä¾‹ï¼š
  $ llmjson process-documents ./documents/                    # ä½¿ç”¨é»˜è®¤è®¾ç½®å¤„ç†
  $ llmjson process-documents ./docs/ -m batch                # ä¼ ç»Ÿæ‰¹é‡æ¨¡å¼
  $ llmjson process-documents ./docs/ -m optimized -o out/    # æµå¼æ¨¡å¼
  $ llmjson process-documents ./docs/ --tables --validation   # æå–è¡¨æ ¼å¹¶éªŒè¯
  $ llmjson process-documents ./docs/ -c prod_config.json -l  # ç”Ÿäº§é…ç½®å¹¶è®°å½•æ—¥å¿—
"""
    docs_parser = subparsers.add_parser('process-documents',
                                      help='Batch process all documents in a folder',
                                      description=docs_help,
                                      formatter_class=argparse.RawDescriptionHelpFormatter)
    docs_parser.add_argument('folder_path',
                           metavar='FOLDER',
                           help='path to folder containing documents')
    docs_parser.add_argument('-c', '--config',
                           metavar='FILE',
                           help='configuration file path (default: config.json)')
    docs_parser.add_argument('-o', '--output',
                           metavar='DIR',
                           help='output directory (default: output)')
    docs_parser.add_argument('-m', '--mode', choices=['batch', 'optimized'],
                           metavar='MODE',
                           help='processing mode: batch or optimized (default: optimized)')
    docs_parser.add_argument('-t', '--template',
                           metavar='FILE',
                           help='custom prompt template file')
    docs_parser.add_argument('--tables', action='store_true',
                           help='extract and process tables from documents')
    docs_parser.add_argument('--validation', action='store_true',
                           help='validate and clean extracted data')
    docs_parser.add_argument('-l', '--log', action='store_true',
                           help='enable detailed console logging')
    docs_parser.set_defaults(func=process_documents_command)

    # éªŒè¯æ•°æ®å‘½ä»¤
    validate_help = """
Validate, repair, and clean JSON knowledge graph data with detailed reporting.

Performs comprehensive validation of extracted knowledge graph data, including
schema validation, data integrity checks, automatic error correction, and
generation of detailed quality reports.

VALIDATION FEATURES:
  â€¢ JSON format validation and repair
  â€¢ Knowledge graph schema verification
  â€¢ Entity and relationship validation
  â€¢ Automatic correction of common errors
  â€¢ Data completeness and consistency checks
  â€¢ Duplicate detection and removal

REPORT CONTENT:
  â€¢ Validation success rate and overall quality score
  â€¢ Error and warning counts with detailed descriptions
  â€¢ Automatic corrections applied
  â€¢ Data statistics (entities, relationships, etc.)
  â€¢ Recommendations for manual review

USAGE:
  llmjson validate INPUT [OPTIONS]

OPTIONS:
  -o, --output FILE         Save validated and cleaned data
  -r, --report FILE         Generate detailed validation report

EXAMPLES:
  $ llmjson validate data.json                              # Basic validation
  $ llmjson validate data.json -o clean.json                # Save cleaned data
  $ llmjson validate data.json -r report.json               # Generate report only
  $ llmjson validate data.json -o clean.json -r report.json # Save both outputs

TYPICAL WORKFLOW:
  1. Process documents:    llmjson process-documents ./docs/
  2. Validate results:     llmjson validate output/*/knowledge_graph.json -o validated.json
  3. Review report:        Check validation metrics and warnings
  4. Use validated data:   Downstream applications use validated.json

éªŒè¯ã€ä¿®å¤å’Œæ¸…ç† JSON çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚

å¯¹æå–çš„çŸ¥è¯†å›¾è°±æ•°æ®è¿›è¡Œå…¨é¢éªŒè¯ï¼ŒåŒ…æ‹¬æ¨¡å¼éªŒè¯ã€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ã€
è‡ªåŠ¨é”™è¯¯ä¿®æ­£å’Œè¯¦ç»†è´¨é‡æŠ¥å‘Šç”Ÿæˆã€‚

éªŒè¯åŠŸèƒ½ï¼š
  â€¢ JSON æ ¼å¼éªŒè¯å’Œä¿®å¤
  â€¢ çŸ¥è¯†å›¾è°±æ¨¡å¼éªŒè¯
  â€¢ å®ä½“å’Œå…³ç³»éªŒè¯
  â€¢ å¸¸è§é”™è¯¯çš„è‡ªåŠ¨ä¿®æ­£
  â€¢ æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§æ£€æŸ¥
  â€¢ é‡å¤æ£€æµ‹å’Œåˆ é™¤

æŠ¥å‘Šå†…å®¹ï¼š
  â€¢ éªŒè¯æˆåŠŸç‡å’Œæ•´ä½“è´¨é‡åˆ†æ•°
  â€¢ é”™è¯¯å’Œè­¦å‘Šè®¡æ•°åŠè¯¦ç»†æè¿°
  â€¢ åº”ç”¨çš„è‡ªåŠ¨ä¿®æ­£
  â€¢ æ•°æ®ç»Ÿè®¡ï¼ˆå®ä½“ã€å…³ç³»ç­‰ï¼‰
  â€¢ äººå·¥å®¡æŸ¥å»ºè®®

ç”¨æ³•ï¼š
  llmjson validate è¾“å…¥ [é€‰é¡¹]

é€‰é¡¹ï¼š
  -o, --output FILE         ä¿å­˜éªŒè¯å’Œæ¸…ç†åçš„æ•°æ®
  -r, --report FILE         ç”Ÿæˆè¯¦ç»†éªŒè¯æŠ¥å‘Š

ç¤ºä¾‹ï¼š
  $ llmjson validate data.json                              # åŸºç¡€éªŒè¯
  $ llmjson validate data.json -o clean.json                # ä¿å­˜æ¸…ç†æ•°æ®
  $ llmjson validate data.json -r report.json               # ä»…ç”ŸæˆæŠ¥å‘Š
  $ llmjson validate data.json -o clean.json -r report.json # ä¿å­˜ä¸¤ä¸ªè¾“å‡º

å…¸å‹å·¥ä½œæµç¨‹ï¼š
  1. å¤„ç†æ–‡æ¡£ï¼š       llmjson process-documents ./docs/
  2. éªŒè¯ç»“æœï¼š       llmjson validate output/*/knowledge_graph.json -o validated.json
  3. å®¡æŸ¥æŠ¥å‘Šï¼š       æ£€æŸ¥éªŒè¯æŒ‡æ ‡å’Œè­¦å‘Š
  4. ä½¿ç”¨éªŒè¯æ•°æ®ï¼š   ä¸‹æ¸¸åº”ç”¨ä½¿ç”¨ validated.json
"""
    validate_parser = subparsers.add_parser('validate',
                                          help='Validate and clean JSON knowledge graph data',
                                          description=validate_help,
                                          formatter_class=argparse.RawDescriptionHelpFormatter)
    validate_parser.add_argument('input',
                              metavar='INPUT',
                              help='input JSON file to validate')
    validate_parser.add_argument('-o', '--output',
                              metavar='FILE',
                              help='save validated and cleaned data to file')
    validate_parser.add_argument('-r', '--report',
                              metavar='FILE',
                              help='generate detailed validation report')
    validate_parser.set_defaults(func=validate_command)
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if not args.command:
        conditional_log(main_logger, 'info', "ğŸ“– æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        parser.print_help()
        return 1
    
    # æ‰§è¡Œå‘½ä»¤
    try:
        conditional_log(main_logger, 'info', f"ğŸ“ æ‰§è¡Œå‘½ä»¤: {args.command}")
        result = args.func(args)
        conditional_log(main_logger, 'info', f"âœ… å‘½ä»¤æ‰§è¡Œå®Œæˆï¼Œé€€å‡ºç : {result}")
        return result
    except KeyboardInterrupt:
        conditional_log(main_logger, 'info', "â¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        print("\nâ¹ï¸  æ“ä½œå·²å–æ¶ˆ")
        return 1
    except Exception as e:
        conditional_log(main_logger, 'error', f"âŒ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        print(f"âŒ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())