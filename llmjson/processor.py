"""LLMå¤„ç†å™¨

æä¾›å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨å’ŒJSONæ•°æ®ç”ŸæˆåŠŸèƒ½ã€‚
"""

import json
import os
import re
import time
from typing import Dict, Any, Optional, List, Tuple, Union
from openai import OpenAI
import json_repair
from concurrent.futures import ThreadPoolExecutor, as_completed

from .prompt_template import PromptTemplate
from .word_chunker import WordChunker
from .log import (
    setup_logging,
    get_logger,
    create_logger_with_context,
    create_timed_logger,
    log_execution_time,
    log_system_info
)
from .exceptions import (
    LLMProcessingError, 
    APIConnectionError, 
    ValidationError
)


class LLMProcessor:
    """å¤§è¯­è¨€æ¨¡å‹å¤„ç†å™¨"""
    
    def __init__(self, 
                 api_key: str,
                 base_url: str = "https://api.openai.com/v1",
                 model: str = "gpt-4o-mini",
                 temperature: float = 0.1,
                 max_tokens: int = 4000,
                 timeout: int = 60,
                 max_retries: int = 3,
                 retry_delay: float = 1.0,
                 max_workers: int = 4,
                 enable_parallel: bool = True,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 stream: bool = False,
                 force_json: bool = True,
                 extra_body: Optional[Dict[str, Any]] = None,
                 prompt_template: Optional[PromptTemplate] = None,
                 word_chunker: Optional[WordChunker] = None):
        """åˆå§‹åŒ–LLMå¤„ç†å™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            enable_parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°ï¼ˆtokenæ•°ï¼‰
            chunk_overlap: æ–‡æ¡£åˆ†å—é‡å å¤§å°ï¼ˆtokenæ•°ï¼‰
            prompt_template: è‡ªå®šä¹‰æç¤ºæ¨¡æ¿å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤
            word_chunker: è‡ªå®šä¹‰æ–‡æ¡£åˆ†å—å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤
        """
        
        # åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        self.logger = create_logger_with_context({
            'component': 'LLMProcessor',
            'model': model,
            'max_workers': max_workers,
            'enable_parallel': enable_parallel
        })
        
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.max_workers = max_workers
        self.enable_parallel = enable_parallel
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.stream = stream
        self.force_json = force_json
        self.extra_body = extra_body
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.prompt_template = prompt_template if prompt_template is not None else PromptTemplate()
        self.word_chunker = word_chunker if word_chunker is not None else WordChunker(max_tokens=chunk_size, overlap_tokens=chunk_overlap)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_used': 0,
            'json_parsing_errors': 0
        }
    
    @log_execution_time()
    def process_chunk(self, 
                     chunk: str, 
                     doc_name: str = "æœªçŸ¥æ–‡æ¡£") -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:
        """å¤„ç†æ–‡æœ¬å—ï¼Œç”ŸæˆçŸ¥è¯†å›¾è°±æ•°æ®
        
        Args:
            chunk: å¾…å¤„ç†çš„æ–‡æœ¬å—
            doc_name: æ–‡æ¡£åç§°
            
        Returns:
            (å¤„ç†ç»“æœ, å¤„ç†ä¿¡æ¯)
            
        Raises:
            LLMProcessingError: å½“å¤„ç†å¤±è´¥æ—¶
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        # åˆ›å»ºå¤„ç†ç‰¹å®šçš„æ—¥å¿—å™¨
        process_logger = create_logger_with_context({
            'operation': 'process_chunk',
            'doc_name': doc_name
        })
        
        try:
            process_logger.debug(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£å—ï¼Œé•¿åº¦: {len(chunk)} å­—ç¬¦")
            
            # åˆ›å»ºæç¤º
            prompt = self._create_prompt(chunk, doc_name)
            process_logger.debug(f"ğŸ“ æç¤ºåˆ›å»ºå®Œæˆï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # è°ƒç”¨LLM API
            reasoning, response = self._call_llm_api(prompt)
            process_logger.debug(f"ğŸ“¡ APIè°ƒç”¨å®Œæˆï¼Œå“åº”é•¿åº¦: {len(response) if response else 0} å­—ç¬¦")
            
            # æå–JSONæ•°æ®
            json_data = self._extract_json(response)

            # ä¸ºæ‰€æœ‰å®ä½“ã€çŠ¶æ€å’Œå…³ç³»æ·»åŠ æ–‡æ¡£æ¥æº
            if json_data is not None:
                self._add_document_source_to_json_data(json_data, doc_name)

            processing_time = time.time() - start_time
            
            if json_data is None:
                self.stats['json_parsing_errors'] += 1
                self.stats['failed_requests'] += 1
                
                error_details = {
                    'success': False,
                    'error': 'JSONè§£æå¤±è´¥',
                    'error_type': 'json_parse_error',
                    'raw_response': response[:1000] if response else None,  # å¢åŠ åˆ°1000å­—ç¬¦ä»¥ä¾¿è°ƒè¯•
                    'raw_response_length': len(response) if response else 0,
                    'processing_time': processing_time,
                    'chunk_length': len(chunk),
                    'reasoning_length': len(reasoning) if reasoning else 0
                }
                
                process_logger.error(f"âŒ JSONè§£æå¤±è´¥")
                process_logger.debug(f"  ğŸ“ å“åº”é•¿åº¦: {len(response) if response else 0}")
                process_logger.debug(f"  ğŸ“„ å“åº”é¢„è§ˆ: {response[:200] if response else 'None'}...")
                
                return None, error_details
            
            # éªŒè¯JSONæ•°æ®è´¨é‡
            validation_result = self._validate_extracted_data(json_data, process_logger)
            
            self.stats['successful_requests'] += 1
            
            success_details = {
                'success': True,
                'model': self.model,
                'chunk_length': len(chunk),
                'response_length': len(response) if response else 0,
                'processing_time': processing_time,
                'reasoning': reasoning,
                'reasoning_length': len(reasoning) if reasoning else 0,
                'validation': validation_result
            }
            
            process_logger.info(f"âœ… å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {processing_time:.2f}s")
            
            return json_data, success_details
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.stats['failed_requests'] += 1
            error_msg = f"å¤„ç†æ–‡æœ¬å—å¤±è´¥: {str(e)}"
            
            process_logger.error(f"âŒ {error_msg}")
            process_logger.debug(f"  â±ï¸ è€—æ—¶: {processing_time:.2f}s")
            
            # æ·»åŠ å¤„ç†æ—¶é—´ä¿¡æ¯åˆ°å¼‚å¸¸ä¸­ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼Œå¯ä»¥åœ¨è°ƒç”¨è€…ä¸­æ•è·å¹¶ä½¿ç”¨ï¼‰
            raise LLMProcessingError(error_msg) from e
    
    def _validate_extracted_data(self, json_data: Dict[str, Any], logger) -> Dict[str, Any]:
        """éªŒè¯æå–çš„JSONæ•°æ®è´¨é‡
        
        Args:
            json_data: æå–çš„JSONæ•°æ®
            logger: æ—¥å¿—å™¨
            
        Returns:
            éªŒè¯ç»“æœä¿¡æ¯
        """
        validation_result = {
            'structure_complete': True,
            'data_quality': 'good',
            'warnings': [],
            'statistics': {}
        }
        
        try:
            if not isinstance(json_data, dict):
                validation_result['warnings'].append('æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼')
                validation_result['data_quality'] = 'poor'
                return validation_result
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            expected_fields = ["åŸºç¡€å®ä½“", "çŠ¶æ€å®ä½“", "çŠ¶æ€å…³ç³»"]
            missing_fields = []
            empty_fields = []
            
            for field in expected_fields:
                if field not in json_data:
                    missing_fields.append(field)
                    validation_result['structure_complete'] = False
                elif not json_data[field] or len(json_data[field]) == 0:
                    empty_fields.append(field)
            
            if missing_fields:
                validation_result['warnings'].append(f'ç¼ºå°‘å­—æ®µ: {missing_fields}')
                validation_result['data_quality'] = 'poor'
                
            if empty_fields:
                validation_result['warnings'].append(f'ç©ºå­—æ®µ: {empty_fields}')
                if validation_result['data_quality'] == 'good':
                    validation_result['data_quality'] = 'fair'
            
            # ç»Ÿè®¡å„å­—æ®µçš„æ•°æ®é‡
            for field in expected_fields:
                if field in json_data and isinstance(json_data[field], list):
                    count = len(json_data[field])
                    validation_result['statistics'][field] = count
                    logger.debug(f"ğŸ“Š {field}: {count} ä¸ªæ¡ç›®")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            total_entities = sum(validation_result['statistics'].values())
            if total_entities == 0:
                validation_result['warnings'].append('æ²¡æœ‰æå–åˆ°ä»»ä½•å®ä½“')
                validation_result['data_quality'] = 'poor'
            elif total_entities < 3:
                validation_result['warnings'].append('æå–çš„å®ä½“æ•°é‡è¾ƒå°‘')
                if validation_result['data_quality'] == 'good':
                    validation_result['data_quality'] = 'fair'
            
            logger.debug(f"âœ… æ•°æ®éªŒè¯å®Œæˆ: {validation_result['data_quality']} è´¨é‡ï¼Œ{total_entities} ä¸ªå®ä½“")
            
        except Exception as e:
            validation_result['warnings'].append(f'éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}')
            validation_result['data_quality'] = 'unknown'
            logger.warning(f"âš ï¸ æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
        
        return validation_result
    
    def _create_prompt(self, chunk: str, doc_name: str) -> List[Dict[str, str]]:
        """åˆ›å»ºæç¤ºï¼ˆç»Ÿä¸€è¿”å› messages æ ¼å¼ï¼‰
        
        Args:
            chunk: æ–‡æœ¬å—
            doc_name: æ–‡æ¡£åç§°
            
        Returns:
            messages åˆ—è¡¨
        """
        try:
            # ç»Ÿä¸€è¿”å› messages æ ¼å¼
            return self.prompt_template.create_prompt(
                chunk=chunk,
                doc_name=doc_name
            )
        except Exception as e:
            raise LLMProcessingError(f"åˆ›å»ºæç¤ºå¤±è´¥: {str(e)}") from e
    
    def _call_llm_api(self, 
                     prompt: List[Dict[str, str]]) -> Tuple[str, str]:
        """è°ƒç”¨LLM API
        
        Args:
            prompt: messages åˆ—è¡¨
            
        Returns:
            LLMæ€è€ƒæ–‡æœ¬, LLMå“åº”æ–‡æœ¬
            
        Raises:
            APIConnectionError: å½“APIè°ƒç”¨å¤±è´¥æ—¶
        """
        # åˆ›å»ºAPIè°ƒç”¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        api_logger = create_logger_with_context({
            'operation': 'api_call',
        })
        
        # prompt å·²ç»æ˜¯ messages æ ¼å¼
        messages = prompt
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "top_p": 0.7,
            "max_tokens": self.max_tokens,
            "timeout": self.timeout,
            "stream": self.stream
        }
        
        # å¦‚æœå¼ºåˆ¶JSONæ ¼å¼
        if self.force_json:
            request_params["response_format"] = {"type": "json_object"}
            
        if self.extra_body:
            request_params["extra_body"] = self.extra_body

        # é‡è¯•æœºåˆ¶
        last_exception = None
        for attempt in range(self.max_retries):
            try:
                api_logger.info(f"ğŸ“¡ å¼€å§‹APIè°ƒç”¨ (å°è¯• {attempt + 1}/{self.max_retries})")
                
                if self.stream or (self.extra_body and self.extra_body.get("enable_thinking", False)):
                    # thinkingæ¨¡å¼æˆ–streamæ¨¡å¼, ä½¿ç”¨æµå¼å“åº”
                    request_params["stream"] = True
                    return self._handle_stream_response(request_params)
                else:
                    response = self.client.chat.completions.create(**request_params)
                    
                    # è®°å½•å“åº”ä¿¡æ¯
                    if hasattr(response, 'usage') and response.usage:
                        self.stats['total_tokens_used'] += response.usage.total_tokens
                        api_logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸ!")
                        api_logger.info(f"  ğŸ“¥ è¾“å…¥Token: {response.usage.prompt_tokens}")
                        api_logger.info(f"  ğŸ“¤ è¾“å‡ºToken: {response.usage.completion_tokens}")
                        api_logger.info(f"  ğŸ“Š æ€»Token: {response.usage.total_tokens}")
                    
                    response_content = response.choices[0].message.content
                    api_logger.info(f"  ğŸ“ å“åº”é•¿åº¦: {len(response_content) if response_content else 0} å­—ç¬¦")
                    api_logger.debug(f"  ğŸ‘€ å“åº”é¢„è§ˆ: {response_content[:200] if response_content else 'None'}{'...' if response_content and len(response_content) > 200 else ''}")
                    
                    return '', response_content
                    
            except Exception as e:
                last_exception = e
                api_logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)}")
                
                if attempt < self.max_retries - 1:
                    sleep_time = self.retry_delay * (2 ** attempt)
                    api_logger.info(f"â³ ç­‰å¾… {sleep_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        error_msg = f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{self.max_retries}æ¬¡: {str(last_exception)}"
        api_logger.error(f"âŒ {error_msg}")
        raise APIConnectionError(error_msg) from last_exception
    
    def _handle_stream_response(self, request_params: Dict[str, Any]) -> Tuple[str, str]:
        """å¤„ç†æµå¼å“åº”
        
        Args:
            request_params: è¯·æ±‚å‚æ•°
            
        Returns:
            å®Œæ•´æ€è€ƒæ–‡æœ¬, å®Œæ•´çš„å“åº”æ–‡æœ¬
        """
        # åˆ›å»ºæµå¼å“åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        stream_logger = create_logger_with_context({
            'operation': 'stream_response',
            'model': self.model
        })
        
        try:
            stream_logger.info("ğŸŒŠ å¼€å§‹å¤„ç†æµå¼å“åº”...")
            stream = self.client.chat.completions.create(**request_params)
            
            # æ”¶é›†ç»“æœ
            reasoning_parts: list[str] = []
            content_parts: list[str] = []
            usage_info = None
            chunk_count = 0
            last_content_chunk = 0  # è®°å½•æœ€åä¸€ä¸ªæœ‰å†…å®¹çš„åˆ†å—å·
            
            for chunk in stream:
                chunk_count += 1
                
                # æ£€æŸ¥chunkç»“æ„
                if not hasattr(chunk, 'choices') or not chunk.choices:
                    stream_logger.debug(f"ğŸ“¥ æ¥æ”¶åˆ†å— #{chunk_count}: æ— choicesæ•°æ®")
                    continue
                    
                delta = chunk.choices[0].delta
                if not delta:
                    stream_logger.debug(f"ğŸ“¥ æ¥æ”¶åˆ†å— #{chunk_count}: æ— deltaæ•°æ®")
                    continue

                # è®°å½•è¯¦ç»†çš„deltaä¿¡æ¯ï¼ˆä»…åœ¨debugçº§åˆ«ï¼‰
                stream_logger.debug(f"ğŸ“¥ æ¥æ”¶åˆ†å— #{chunk_count}: {delta}")
                
                # 1. æ€è€ƒå†…å®¹ï¼ˆreasoning/reasoning_contentï¼‰
                if hasattr(delta, "reasoning") and delta.reasoning:
                    reasoning_parts.append(delta.reasoning)
                    stream_logger.debug(f"ğŸ§  åˆ†å— #{chunk_count}: æ€è€ƒå†…å®¹ {len(delta.reasoning)} å­—ç¬¦")
                elif hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    reasoning_parts.append(delta.reasoning_content)
                    stream_logger.debug(f"ğŸ§  åˆ†å— #{chunk_count}: æ€è€ƒå†…å®¹ {len(delta.reasoning_content)} å­—ç¬¦")
                
                # 2. æ­£å¼å›å¤ï¼ˆcontentï¼‰
                if delta.content:
                    content_parts.append(delta.content)
                    last_content_chunk = chunk_count
                    
                # 3. usageä¿¡æ¯æ”¶é›† - æ›´å…¨é¢çš„å¤„ç†
                if hasattr(chunk, "usage") and chunk.usage:
                    usage_info = chunk.usage
                    # stream_logger.debug(f"ğŸ“Š åˆ†å— #{chunk_count}: æ”¶åˆ°usageä¿¡æ¯ {usage_info}")

            # å¤„ç†usageç»Ÿè®¡
            if usage_info:
                total_tokens = usage_info.total_tokens if hasattr(usage_info, 'total_tokens') else 0
                prompt_tokens = usage_info.prompt_tokens if hasattr(usage_info, 'prompt_tokens') else 0
                completion_tokens = usage_info.completion_tokens if hasattr(usage_info, 'completion_tokens') else 0
                
                self.stats['total_tokens_used'] += total_tokens
                
                stream_logger.info(f"ğŸ“Š Tokenç»Ÿè®¡ - è¾“å…¥: {prompt_tokens}, è¾“å‡º: {completion_tokens}, æ€»è®¡: {total_tokens}")
            else:
                stream_logger.warning("âš ï¸ æµå¼å“åº”ä¸­æœªæ”¶åˆ°usageä¿¡æ¯")

            full_reasoning = ''.join(reasoning_parts)
            full_content = ''.join(content_parts)

            # è®°å½•æµå¼å“åº”ç»Ÿè®¡
            stream_logger.info(f"âœ… æµå¼å“åº”å¤„ç†å®Œæˆ!")
            stream_logger.info(f"  ğŸ“Š æ¥æ”¶åˆ†å—æ•°: {chunk_count}")
            stream_logger.info(f"  ğŸ“ æœ€åå†…å®¹åˆ†å—: #{last_content_chunk}")
            stream_logger.info(f"  ğŸ§  æ¨ç†å†…å®¹é•¿åº¦: {len(full_reasoning)} å­—ç¬¦")
            stream_logger.info(f"  ğŸ’¬ å“åº”å†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦è¢«æˆªæ–­
            if chunk_count > 0 and last_content_chunk < chunk_count - 50:  # å¦‚æœæœ€å50ä¸ªåˆ†å—éƒ½æ²¡æœ‰å†…å®¹ï¼Œå¯èƒ½è¢«æˆªæ–­
                stream_logger.warning(f"âš ï¸ å¯èƒ½çš„å†…å®¹æˆªæ–­ï¼šæœ€åå†…å®¹åˆ†å— #{last_content_chunk}ï¼Œæ€»åˆ†å— #{chunk_count}")
            
            # æ£€æŸ¥å“åº”å®Œæ•´æ€§
            if full_content:
                if not (full_content.rstrip().endswith('}') or full_content.rstrip().endswith(']')):
                    stream_logger.warning("âš ï¸ å“åº”å†…å®¹å¯èƒ½ä¸å®Œæ•´ï¼Œæœªä»¥}æˆ–]ç»“å°¾")
            
            return full_reasoning, full_content

        except Exception as e:
            stream_logger.error(f"âŒ æµå¼å“åº”å¤„ç†å¤±è´¥: {str(e)}")
            raise APIConnectionError(f"æµå¼å“åº”å¤„ç†å¤±è´¥: {str(e)}") from e
    
    def _extract_json(self, response: str) -> Optional[Dict[str, Any]]:
        """ä»LLMå“åº”ä¸­æå–JSONæ•°æ®
        
        Args:
            response: LLMå“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„JSONæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not response:
            return None
        
        # åˆ›å»ºJSONæå–ç‰¹å®šçš„æ—¥å¿—å™¨
        extract_logger = create_logger_with_context({
            'operation': 'json_extraction'
        })
        
        extract_logger.debug(f"ğŸ” å¼€å§‹JSONæå–ï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        extract_logger.debug(f"ğŸ“„ å“åº”é¢„è§ˆ: {response[:500]}{'...' if len(response) > 500 else ''}")
        
        # 1. é¦–å…ˆå°è¯•ç›´æ¥è§£æ
        try:
            result = json.loads(response)
            extract_logger.info("âœ… ç›´æ¥JSONè§£ææˆåŠŸ")
            return self._validate_json_structure(result, extract_logger)
        except json.JSONDecodeError as e:
            extract_logger.debug(f"âŒ ç›´æ¥JSONè§£æå¤±è´¥: {str(e)}")
        
        # 2. å°è¯•æå–JSONä»£ç å—
        extract_logger.debug("ğŸ” å°è¯•æå–JSONä»£ç å—...")
        json_patterns = [
            r'```(?:json)?\s*({.*?})\s*```',  # æ ‡å‡†ä»£ç å—
            r'```(?:json)?\s*(\[.*?\])\s*```'  # æ•°ç»„ä»£ç å—
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            extract_logger.debug(f"ğŸ“‹ ä»£ç å—æ¨¡å¼åŒ¹é…åˆ° {len(matches)} ä¸ªç»“æœ")
            
            for i, match in enumerate(matches):
                try:
                    result = json.loads(match)
                    extract_logger.info(f"âœ… JSONä»£ç å—è§£ææˆåŠŸ (ç¬¬{i+1}ä¸ª)")
                    return self._validate_json_structure(result, extract_logger)
                except json.JSONDecodeError as e:
                    extract_logger.debug(f"âŒ ä»£ç å— {i+1} è§£æå¤±è´¥: {str(e)}")
                    continue
        
        # 3. æ™ºèƒ½æŸ¥æ‰¾JSONå¯¹è±¡ - ä½¿ç”¨æ›´ç²¾ç¡®çš„åµŒå¥—åŒ¹é…
        extract_logger.debug("ğŸ” å°è¯•æ™ºèƒ½JSONå¯¹è±¡æå–...")
        json_candidates = self._find_json_candidates(response, extract_logger)
        
        for i, candidate in enumerate(json_candidates):
            try:
                result = json.loads(candidate)
                extract_logger.info(f"âœ… JSONå¯¹è±¡è§£ææˆåŠŸ (å€™é€‰é¡¹{i+1})")
                return self._validate_json_structure(result, extract_logger)
            except json.JSONDecodeError as e:
                extract_logger.debug(f"âŒ å€™é€‰é¡¹ {i+1} è§£æå¤±è´¥: {str(e)}")
                continue
        
        # 4. ä½¿ç”¨json_repairå°è¯•ä¿®å¤
        extract_logger.debug("ğŸ”§ å°è¯•JSONä¿®å¤...")
        repair_attempts = [
            response,  # å®Œæ•´å“åº”
            *json_candidates  # æ‰€æœ‰å€™é€‰é¡¹
        ]
        
        for i, content in enumerate(repair_attempts):
            if not content.strip():
                continue
                
            try:
                # é¢„å¤„ç†ï¼šå°è¯•ä¿®å¤å¸¸è§çš„æˆªæ–­é—®é¢˜
                processed_content = self._preprocess_for_repair(content, extract_logger)
                repaired = json_repair.repair_json(processed_content)
                result = json.loads(repaired)
                extract_logger.info(f"âœ… JSONä¿®å¤æˆåŠŸ (å°è¯•{i+1})")
                return self._validate_json_structure(result, extract_logger)
            except Exception as e:
                extract_logger.debug(f"âŒ ä¿®å¤å°è¯• {i+1} å¤±è´¥: {str(e)}")
                continue
        
        extract_logger.error("âŒ æ‰€æœ‰JSONæå–æ–¹æ³•éƒ½å¤±è´¥äº†")
        return None
    
    def _find_json_candidates(self, text: str, logger) -> List[str]:
        """æ™ºèƒ½æŸ¥æ‰¾JSONå€™é€‰é¡¹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            logger: æ—¥å¿—å™¨
            
        Returns:
            JSONå€™é€‰é¡¹åˆ—è¡¨ï¼ŒæŒ‰è´¨é‡æ’åº
        """
        candidates = []
        
        # æ–¹æ³•1: æŸ¥æ‰¾å®Œæ•´çš„å¤§æ‹¬å·åŒ…å›´çš„å†…å®¹
        brace_candidates = self._extract_balanced_braces(text)
        candidates.extend(brace_candidates)
        
        # æ–¹æ³•2: æŸ¥æ‰¾ä»ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } çš„å†…å®¹
        first_brace = text.find('{')
        last_brace = text.rfind('}')
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            full_candidate = text[first_brace:last_brace + 1]
            if full_candidate not in candidates:
                candidates.append(full_candidate)
        
        # æ–¹æ³•3: æŸ¥æ‰¾æ•°ç»„æ ¼å¼ [ ... ]
        first_bracket = text.find('[')
        last_bracket = text.rfind(']')
        if first_bracket != -1 and last_bracket != -1 and last_bracket > first_bracket:
            array_candidate = text[first_bracket:last_bracket + 1]
            if array_candidate not in candidates:
                candidates.append(array_candidate)
        
        logger.debug(f"ğŸ“‹ æ‰¾åˆ° {len(candidates)} ä¸ªJSONå€™é€‰é¡¹")
        
        # æŒ‰é•¿åº¦å’Œå®Œæ•´æ€§æ’åº
        candidates.sort(key=lambda x: (len(x), x.count('{'), x.count('}')), reverse=True)
        
        return candidates[:10]  # é™åˆ¶å€™é€‰é¡¹æ•°é‡ï¼Œé¿å…è¿‡å¤šå°è¯•
    
    def _extract_balanced_braces(self, text: str) -> List[str]:
        """æå–å¹³è¡¡å¤§æ‹¬å·çš„JSONå†…å®¹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¹³è¡¡æ‹¬å·çš„å†…å®¹åˆ—è¡¨
        """
        candidates = []
        brace_count = 0
        start_pos = -1
        
        for i, char in enumerate(text):
            if char == '{':
                if brace_count == 0:
                    start_pos = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_pos != -1:
                    candidate = text[start_pos:i + 1]
                    if len(candidate) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                        candidates.append(candidate)
                    start_pos = -1
        
        return candidates
    
    def _preprocess_for_repair(self, content: str, logger) -> str:
        """é¢„å¤„ç†å†…å®¹ä»¥æé«˜JSONä¿®å¤æˆåŠŸç‡
        
        Args:
            content: åŸå§‹å†…å®¹
            logger: æ—¥å¿—å™¨
            
        Returns:
            é¢„å¤„ç†åçš„å†…å®¹
        """
        original_length = len(content)
        
        # 1. ç§»é™¤å¯èƒ½çš„éJSONå‰ç¼€å’Œåç¼€
        content = content.strip()
        
        # 2. æ£€æŸ¥æ˜¯å¦ä»¥ { å¼€å§‹ï¼Œå¦‚æœä¸æ˜¯ï¼Œå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª {
        if not content.startswith('{') and not content.startswith('['):
            first_brace = content.find('{')
            first_bracket = content.find('[')
            
            start_pos = -1
            if first_brace != -1 and first_bracket != -1:
                start_pos = min(first_brace, first_bracket)
            elif first_brace != -1:
                start_pos = first_brace
            elif first_bracket != -1:
                start_pos = first_bracket
            
            if start_pos != -1:
                content = content[start_pos:]
        
        # 3. æ£€æŸ¥æ˜¯å¦ä»¥ } æˆ– ] ç»“æŸï¼Œå¦‚æœä¸æ˜¯ï¼Œå¯èƒ½è¢«æˆªæ–­
        if not content.endswith('}') and not content.endswith(']'):
            logger.debug("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„JSONæˆªæ–­")
            
            # å°è¯•æ™ºèƒ½è¡¥å…¨
            if content.startswith('{'):
                # ç»Ÿè®¡æ‹¬å·å¹³è¡¡
                open_braces = content.count('{')
                close_braces = content.count('}')
                missing_braces = open_braces - close_braces
                
                if missing_braces > 0:
                    # æ£€æŸ¥æœ€åæ˜¯å¦æœ‰æœªå®Œæˆçš„å­—ç¬¦ä¸²
                    if content.rstrip().endswith('"') or content.rstrip().endswith(','):
                        content = content.rstrip().rstrip(',') + '}'
                    else:
                        content += '}' * missing_braces
                    logger.debug(f"ğŸ”§ è¡¥å…¨äº† {missing_braces} ä¸ªé—­åˆæ‹¬å·")
            
            elif content.startswith('['):
                # å¤„ç†æ•°ç»„æˆªæ–­
                open_brackets = content.count('[')
                close_brackets = content.count(']')
                missing_brackets = open_brackets - close_brackets
                
                if missing_brackets > 0:
                    content += ']' * missing_brackets
                    logger.debug(f"ğŸ”§ è¡¥å…¨äº† {missing_brackets} ä¸ªé—­åˆä¸­æ‹¬å·")
        
        if len(content) != original_length:
            logger.debug(f"ğŸ“ é¢„å¤„ç†ï¼šé•¿åº¦ä» {original_length} å˜ä¸º {len(content)}")
        
        return content
    
    def _validate_json_structure(self, json_data: Dict[str, Any], logger) -> Optional[Dict[str, Any]]:
        """éªŒè¯JSONç»“æ„çš„å®Œæ•´æ€§
        
        Args:
            json_data: è§£æåçš„JSONæ•°æ®
            logger: æ—¥å¿—å™¨
            
        Returns:
            éªŒè¯é€šè¿‡çš„JSONæ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        if not isinstance(json_data, dict):
            logger.warning("âš ï¸ JSONæ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼")
            return json_data  # ä»ç„¶è¿”å›ï¼Œå¯èƒ½æ˜¯æ•°ç»„æ ¼å¼
        
        # æ£€æŸ¥å¿…éœ€çš„é¡¶çº§å­—æ®µ
        expected_fields = ["åŸºç¡€å®ä½“", "çŠ¶æ€å®ä½“", "çŠ¶æ€å…³ç³»"]
        missing_fields = []
        
        for field in expected_fields:
            if field not in json_data:
                missing_fields.append(field)
        
        if missing_fields:
            logger.warning(f"âš ï¸ JSONç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘å­—æ®µ: {missing_fields}")
            # ä¸è¿”å›Noneï¼Œä»ç„¶æ¥å—éƒ¨åˆ†æ•°æ®ï¼Œè®©ä¸Šå±‚å†³å®šå¦‚ä½•å¤„ç†
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        total_entities = 0
        for field in expected_fields:
            if field in json_data and isinstance(json_data[field], list):
                count = len(json_data[field])
                total_entities += count
                logger.debug(f"ğŸ“Š {field}: {count} ä¸ªæ¡ç›®")
        
        logger.info(f"âœ… JSONç»“æ„éªŒè¯å®Œæˆï¼Œæ€»è®¡ {total_entities} ä¸ªå®ä½“")
        
        return json_data
    
    @log_execution_time()
    def batch_process(self, 
                     chunk_items: List[Tuple[str, int, str]], 
                     **kwargs) -> List[Tuple[Optional[Dict[str, Any]], Dict[str, Any]]]:
        """æ‰¹é‡å¤„ç†æ–‡æœ¬å—
        
        Args:
            chunk_items: åŒ…å«(doc_name, chunk_index, chunk_content)çš„å…ƒç»„åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨ï¼Œä¿æŒä¸è¾“å…¥ç›¸åŒçš„é¡ºåº
        """
        if not self.enable_parallel or len(chunk_items) <= 1:
            # ä¸²è¡Œå¤„ç†
            return self._batch_process_serial(chunk_items, **kwargs)
        else:
            # å¹¶è¡Œå¤„ç†
            return self._batch_process_parallel(chunk_items, **kwargs)
    
    def _batch_process_serial(self, 
                             chunk_items: List[Tuple[str, int, str]], 
                             **kwargs) -> List[Tuple[Optional[Dict[str, Any]], Dict[str, Any]]]:
        """ä¸²è¡Œæ‰¹é‡å¤„ç†æ–‡æœ¬å—
        
        Args:
            chunk_items: åŒ…å«(doc_name, chunk_index, chunk_content)çš„å…ƒç»„åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i, (doc_name, chunk_index, chunk) in enumerate(chunk_items):
            start_time = time.time()
            try:
                process_logger = create_logger_with_context({
                    'operation': 'process_serial'
                })
                process_logger.debug(f"ğŸ”„ å¤„ç†æ–‡æ¡£ '{doc_name}' çš„åˆ†å— #{chunk_index}")
                result = self.process_chunk(chunk, doc_name, **kwargs)
                # åœ¨ç»“æœä¸­æ·»åŠ æ–‡æ¡£å’Œåˆ†å—ä¿¡æ¯
                # if result[1].get('success', False):
                result[1]['doc_name'] = doc_name
                result[1]['chunk_index'] = chunk_index
                result[1]['global_index'] = i
                results.append(result)
            except Exception as e:
                processing_time = time.time() - start_time
                error_info = {
                    'success': False,
                    'error': str(e),
                    'doc_name': doc_name,
                    'chunk_index': chunk_index,
                    'global_index': i,
                    'processing_time': processing_time
                }
                results.append((None, error_info))
        
        return results
    
    def _batch_process_parallel(self, 
                               chunk_items: List[Tuple[str, int, str]], 
                               **kwargs) -> List[Tuple[Optional[Dict[str, Any]], Dict[str, Any]]]:
        """å¹¶è¡Œæ‰¹é‡å¤„ç†æ–‡æœ¬å—
        
        Args:
            chunk_items: åŒ…å«(doc_name, chunk_index, chunk_content)çš„å…ƒç»„åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = [None] * len(chunk_items)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨ï¼Œä¿æŒé¡ºåº
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(self._process_chunk_with_metadata, i, doc_name, chunk_index, chunk, **kwargs): i 
                for i, (doc_name, chunk_index, chunk) in enumerate(chunk_items)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                global_index = future_to_index[future]
                try:
                    results[global_index] = future.result()
                except Exception as e:
                    doc_name, chunk_index, _ = chunk_items[global_index]
                    error_info = {
                        'success': False,
                        'error': str(e),
                        'doc_name': doc_name,
                        'chunk_index': chunk_index,
                        'global_index': global_index,
                        'processing_time': None  # åœ¨å¹¶è¡Œå¤„ç†çš„å¼‚å¸¸æƒ…å†µä¸‹ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—å¤„ç†æ—¶é—´
                    }
                    results[global_index] = (None, error_info)
        
        return results
    
    def _process_chunk_with_metadata(self, 
                                    global_index: int,
                                    doc_name: str,
                                    chunk_index: int,
                                    chunk: str, 
                                    **kwargs) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:
        """å¸¦å…ƒæ•°æ®çš„æ–‡æœ¬å—å¤„ç†ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰
        
        Args:
            global_index: å…¨å±€ç´¢å¼•
            doc_name: æ–‡æ¡£åç§°
            chunk_index: æ–‡æ¡£å†…åˆ†å—ç´¢å¼•
            chunk: æ–‡æœ¬å—å†…å®¹
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()
        try:
            process_logger = create_logger_with_context({
                'operation': 'process_parallel'
            })
            process_logger.debug(f"ğŸ”„ å¤„ç†æ–‡æ¡£ '{doc_name}' çš„åˆ†å— #{chunk_index} (å…¨å±€ç´¢å¼• #{global_index})")
            result = self.process_chunk(chunk, doc_name, **kwargs)
            # åœ¨ç»“æœä¸­æ·»åŠ æ–‡æ¡£å’Œåˆ†å—ä¿¡æ¯
            # if result[1].get('success', False):
            result[1]['doc_name'] = doc_name
            result[1]['chunk_index'] = chunk_index
            result[1]['global_index'] = global_index
            process_logger.debug(f"âœ… å¤„ç†å®Œæˆæ–‡æ¡£ '{doc_name}' çš„åˆ†å— #{chunk_index} (å…¨å±€ç´¢å¼• #{global_index})")
            return result
        except Exception as e:
            processing_time = time.time() - start_time
            error_info = {
                'success': False,
                'error': str(e),
                'doc_name': doc_name,
                'chunk_index': chunk_index,
                'global_index': global_index,
                'processing_time': processing_time
            }
            process_logger.error(f"âŒ å¤„ç†æ–‡æ¡£ '{doc_name}' çš„åˆ†å— #{chunk_index} å¤±è´¥: {str(e)}")
            return (None, error_info)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = self.stats.copy()
        
        # è®¡ç®—æˆåŠŸç‡
        if stats['total_requests'] > 0:
            stats['success_rate'] = stats['successful_requests'] / stats['total_requests'] * 100
        else:
            stats['success_rate'] = 0.0
        
        # è®¡ç®—å¹³å‡tokenä½¿ç”¨é‡
        if stats['successful_requests'] > 0:
            stats['avg_tokens_per_request'] = stats['total_tokens_used'] / stats['successful_requests']
        else:
            stats['avg_tokens_per_request'] = 0.0
        
        return stats
    
    @log_execution_time()
    def process_documents(self, 
                         doc_paths: List[str],
                         include_tables: bool = True,
                         **kwargs) -> Dict[str, List[Tuple[Optional[Dict[str, Any]], Dict[str, Any]]]]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£åˆ—è¡¨
        
        Args:
            doc_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Returns:
            ä»¥æ–‡æ¡£è·¯å¾„ä¸ºé”®ï¼Œå¤„ç†ç»“æœåˆ—è¡¨ä¸ºå€¼çš„å­—å…¸
        """
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£çš„åˆ†å—ä¿¡æ¯
        all_chunk_items = []
        doc_chunk_mapping = {}  # è®°å½•æ¯ä¸ªæ–‡æ¡£çš„åˆ†å—åœ¨å…¨å±€åˆ—è¡¨ä¸­çš„ä½ç½®
        
        for doc_path in doc_paths:
            try:
                # è·å–æ–‡æ¡£åç§°
                doc_name = os.path.basename(doc_path)
                
                # åˆ†å—å¤„ç†æ–‡æ¡£
                if doc_path.lower().endswith(('.docx', '.doc')):
                    # Wordæ–‡æ¡£åˆ†å—
                    chunks = self.word_chunker.chunk_document_with_tables(doc_path) if include_tables else self.word_chunker.chunk_document(doc_path)
                else:
                    # çº¯æ–‡æœ¬æ–‡æ¡£åˆ†å—
                    with open(doc_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    chunks = self._chunk_text(text)
                
                # è®°å½•å½“å‰æ–‡æ¡£åˆ†å—çš„èµ·å§‹ä½ç½®
                start_index = len(all_chunk_items)
                
                # å°†åˆ†å—æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
                for chunk_index, chunk in enumerate(chunks):
                    all_chunk_items.append((doc_name, chunk_index, chunk))
                
                # è®°å½•æ–‡æ¡£åˆ†å—æ˜ å°„
                doc_chunk_mapping[doc_path] = {
                    'start_index': start_index,
                    'chunk_count': len(chunks),
                    'doc_name': doc_name
                }
                
            except Exception as e:
                # è®°å½•æ–‡æ¡£å¤„ç†é”™è¯¯
                error_info = {
                    'success': False,
                    'error': f'æ–‡æ¡£åˆ†å—å¤±è´¥: {str(e)}',
                    'doc_path': doc_path
                }
                doc_chunk_mapping[doc_path] = {
                    'start_index': len(all_chunk_items),
                    'chunk_count': 1,
                    'doc_name': os.path.basename(doc_path),
                    'error': error_info
                }
                # æ·»åŠ é”™è¯¯å ä½ç¬¦
                all_chunk_items.append((os.path.basename(doc_path), 0, ""))
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰åˆ†å—
        if all_chunk_items:
            all_results = self.batch_process(all_chunk_items, **kwargs)
        else:
            all_results = []
        
        # å°†ç»“æœæŒ‰æ–‡æ¡£é‡æ–°ç»„ç»‡
        results = {}
        for doc_path, mapping in doc_chunk_mapping.items():
            start_idx = mapping['start_index']
            chunk_count = mapping['chunk_count']
            
            if 'error' in mapping:
                # æ–‡æ¡£åˆ†å—å¤±è´¥çš„æƒ…å†µ
                results[doc_path] = [(None, mapping['error'])]
            else:
                # æå–è¯¥æ–‡æ¡£çš„å¤„ç†ç»“æœ
                doc_results = all_results[start_idx:start_idx + chunk_count]
                results[doc_path] = doc_results
        
        return results
    
    @log_execution_time()
    def process_documents_streaming(self, 
                                   doc_paths: List[str],
                                   include_tables: bool = True,
                                   callback=None,
                                   **kwargs):
        """æµå¼å¤„ç†æ–‡æ¡£åˆ—è¡¨ï¼Œé¿å…å†…å­˜ç§¯ç´¯
        
        Args:
            doc_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(doc_path, doc_results)å‚æ•°
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Yields:
            (doc_path, doc_results) å…ƒç»„
        """
        for doc_path in doc_paths:
            try:
                # è·å–æ–‡æ¡£åç§°
                doc_name = os.path.basename(doc_path)
                
                # åˆ†å—å¤„ç†æ–‡æ¡£
                if doc_path.lower().endswith(('.docx', '.doc')):
                    # Wordæ–‡æ¡£åˆ†å—
                    chunks = self.word_chunker.chunk_document_with_tables(doc_path) if include_tables else self.word_chunker.chunk_document(doc_path)
                else:
                    # çº¯æ–‡æœ¬æ–‡æ¡£åˆ†å—
                    with open(doc_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    chunks = self._chunk_text(text)
                
                # å‡†å¤‡åˆ†å—é¡¹ç›®
                chunk_items = [(doc_name, chunk_index, chunk) for chunk_index, chunk in enumerate(chunks)]
                
                # å¤„ç†å½“å‰æ–‡æ¡£çš„æ‰€æœ‰åˆ†å—
                doc_results = self.batch_process(chunk_items, **kwargs)
                
                # è¿”å›ç»“æœ
                result = (doc_path, doc_results)
                
                # å¦‚æœæœ‰å›è°ƒå‡½æ•°ï¼Œè°ƒç”¨å®ƒ
                if callback:
                    callback(doc_path, doc_results)
                
                yield result
                
            except Exception as e:
                # å¤„ç†æ–‡æ¡£é”™è¯¯
                error_info = {
                    'success': False,
                    'error': f'æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}',
                    'doc_path': doc_path
                }
                error_result = (doc_path, [(None, error_info)])
                
                if callback:
                    callback(doc_path, [(None, error_info)])
                
                yield error_result
    
    @log_execution_time()
    def process_documents_streaming_optimized(self, 
                                            doc_paths: List[str],
                                            include_tables: bool = True,
                                            callback=None,
                                            batch_size: Optional[int] = None,
                                            **kwargs):
        """ä¼˜åŒ–çš„æµå¼å¤„ç†æ–‡æ¡£åˆ—è¡¨ï¼Œå……åˆ†åˆ©ç”¨çº¿ç¨‹èµ„æº
        
        é€šè¿‡è·¨æ–‡æ¡£æ”¶é›†åˆ†å—åˆ°ç¼“å†²æ± ï¼Œç¡®ä¿æ¯æ¬¡æ‰¹å¤„ç†éƒ½èƒ½å……åˆ†åˆ©ç”¨æ‰€æœ‰çº¿ç¨‹ã€‚
        å½“æ–‡æ¡£å¤„ç†å®Œæˆæ—¶ç«‹å³è¿”å›ç»“æœï¼Œä¿æŒæµå¼ç‰¹æ€§ã€‚
        
        Args:
            doc_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            include_tables: æ˜¯å¦åŒ…å«è¡¨æ ¼å¤„ç†
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(doc_path, doc_results)å‚æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸ºmax_workersçš„2å€
            **kwargs: ä¼ é€’ç»™process_chunkçš„é¢å¤–å‚æ•°
            
        Yields:
            (doc_path, doc_results) å…ƒç»„
        """
        from collections import deque
        
        # ç¡®å®šæ‰¹å¤„ç†å¤§å°
        if batch_size is None:
            batch_size = max(self.max_workers * 2, 8)  # é»˜è®¤ä¸ºçº¿ç¨‹æ•°çš„2å€ï¼Œæœ€å°‘8ä¸ª
        
        # æ–‡æ¡£åˆ†å—ç”Ÿæˆå™¨
        def chunk_generator():
            for doc_path in doc_paths:
                try:
                    # è·å–æ–‡æ¡£åç§°
                    doc_name = os.path.basename(doc_path)
                    
                    # åˆ†å—å¤„ç†æ–‡æ¡£
                    if doc_path.lower().endswith(('.docx', '.doc')):
                        chunks = self.word_chunker.chunk_document_with_tables(doc_path) if include_tables else self.word_chunker.chunk_document(doc_path)
                    else:
                        with open(doc_path, 'r', encoding='utf-8') as f:
                            text = f.read()
                        chunks = self._chunk_text(text)
                    
                    # ç”Ÿæˆåˆ†å—é¡¹ç›®
                    for chunk_index, chunk in enumerate(chunks):
                        yield {
                            'doc_path': doc_path,
                            'doc_name': doc_name,
                            'chunk_index': chunk_index,
                            'chunk': chunk,
                            'chunk_item': (doc_name, chunk_index, chunk)
                        }
                    
                    # æ ‡è®°æ–‡æ¡£ç»“æŸ
                    yield {
                        'doc_path': doc_path,
                        'doc_name': doc_name,
                        'is_doc_end': True
                    }
                    
                except Exception as e:
                    # æ–‡æ¡£å¤„ç†é”™è¯¯
                    yield {
                        'doc_path': doc_path,
                        'doc_name': os.path.basename(doc_path),
                        'error': f'æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}',
                        'is_doc_end': True
                    }
        
        # åˆå§‹åŒ–çŠ¶æ€
        chunk_buffer = []  # åˆ†å—ç¼“å†²æ± 
        pending_docs = {}  # å¾…å®Œæˆçš„æ–‡æ¡£ {doc_path: {'chunks': [], 'total_chunks': int}}
        chunk_gen = chunk_generator()
        
        try:
            while True:
                # å¡«å……ç¼“å†²æ± ç›´åˆ°è¾¾åˆ°æ‰¹å¤„ç†å¤§å°æˆ–æ²¡æœ‰æ›´å¤šåˆ†å—
                while len(chunk_buffer) < batch_size:
                    try:
                        chunk_info = next(chunk_gen)
                        
                        if chunk_info.get('is_doc_end'):
                            # æ–‡æ¡£ç»“æŸæ ‡è®°
                            doc_path = chunk_info['doc_path']
                            
                            if chunk_info.get('error'):
                                # æ–‡æ¡£å¤„ç†é”™è¯¯
                                error_info = {
                                    'success': False,
                                    'error': chunk_info['error'],
                                    'doc_path': doc_path
                                }
                                error_result = (doc_path, [(None, error_info)])
                                
                                if callback:
                                    callback(doc_path, [(None, error_info)])
                                
                                yield error_result
                            else:
                                # æ­£å¸¸æ–‡æ¡£ç»“æŸï¼Œæ ‡è®°æ€»åˆ†å—æ•°
                                if doc_path in pending_docs:
                                    pending_docs[doc_path]['is_complete'] = True
                        else:
                            # æ™®é€šåˆ†å—
                            doc_path = chunk_info['doc_path']
                            
                            # æ·»åŠ åˆ°ç¼“å†²æ± 
                            chunk_buffer.append(chunk_info)
                            
                            # åˆå§‹åŒ–æˆ–æ›´æ–°å¾…å®Œæˆæ–‡æ¡£ä¿¡æ¯
                            if doc_path not in pending_docs:
                                pending_docs[doc_path] = {
                                    'chunks': [],
                                    'chunk_results': {},
                                    'is_complete': False
                                }
                            
                    except StopIteration:
                        # æ²¡æœ‰æ›´å¤šåˆ†å—
                        break
                
                # å¦‚æœç¼“å†²æ± ä¸ºç©ºï¼Œè¯´æ˜æ‰€æœ‰åˆ†å—éƒ½å¤„ç†å®Œäº†
                if not chunk_buffer:
                    break
                
                # æ‰¹é‡å¤„ç†ç¼“å†²æ± ä¸­çš„åˆ†å—
                chunk_items = [info['chunk_item'] for info in chunk_buffer]
                batch_results = self.batch_process(chunk_items, **kwargs)
                
                # å°†ç»“æœåˆ†é…ç»™å¯¹åº”çš„æ–‡æ¡£
                for i, chunk_info in enumerate(chunk_buffer):
                    doc_path = chunk_info['doc_path']
                    chunk_index = chunk_info['chunk_index']
                    result = batch_results[i]
                    
                    # å­˜å‚¨ç»“æœ
                    pending_docs[doc_path]['chunk_results'][chunk_index] = result
                
                # æ¸…ç©ºç¼“å†²æ± 
                chunk_buffer = []
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£å¯ä»¥å®Œæˆå¹¶è¿”å›
                completed_docs = []
                for doc_path, doc_info in pending_docs.items():
                    if doc_info['is_complete']:
                        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†å—éƒ½å·²å¤„ç†
                        chunk_results = doc_info['chunk_results']
                        if chunk_results:  # ç¡®ä¿æœ‰åˆ†å—ç»“æœ
                            # æŒ‰åˆ†å—ç´¢å¼•æ’åºç»“æœ
                            sorted_results = [chunk_results[i] for i in sorted(chunk_results.keys())]
                            
                            result = (doc_path, sorted_results)
                            
                            if callback:
                                callback(doc_path, sorted_results)
                            
                            yield result
                            completed_docs.append(doc_path)
                
                # ç§»é™¤å·²å®Œæˆçš„æ–‡æ¡£
                for doc_path in completed_docs:
                    del pending_docs[doc_path]
                
        except Exception as e:
            # å¤„ç†æ„å¤–é”™è¯¯
            for doc_path in pending_docs.keys():
                error_info = {
                    'success': False,
                    'error': f'æ‰¹å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}',
                    'doc_path': doc_path
                }
                error_result = (doc_path, [(None, error_info)])
                
                if callback:
                    callback(doc_path, [(None, error_info)])
                
                yield error_result
    
    def _chunk_text(self, text: str) -> List[str]:
        """å°†çº¯æ–‡æœ¬åˆ†å—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n')
        
        for para in paragraphs:
            if not para.strip():
                continue
                
            para_with_newline = para + '\n'
            para_tokens = self.word_chunker.estimate_tokens(para_with_newline)
            
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šå·²æœ‰å†…å®¹ä¼šè¶…è¿‡æœ€å¤§tokenæ•°ï¼Œåˆ™ç»“æŸå½“å‰åˆ†å—
            if current_tokens + para_tokens > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                # ä¿ç•™é‡å éƒ¨åˆ†ä½œä¸ºæ–°åˆ†å—çš„å¼€å§‹
                overlap_text = self.word_chunker._get_overlap_text(current_chunk)
                current_chunk = overlap_text + para_with_newline
                current_tokens = self.word_chunker.estimate_tokens(current_chunk)
            else:
                current_chunk += para_with_newline
                current_tokens += para_tokens
        
        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    def _add_document_source_to_json_data(self, json_data: Dict[str, Any], doc_name: str) -> None:
        """ä¸ºJSONæ•°æ®ä¸­çš„æ‰€æœ‰å®ä½“ã€çŠ¶æ€å’Œå…³ç³»æ·»åŠ æ–‡æ¡£æ¥æº

        Args:
            json_data: LLMè¿”å›çš„JSONæ•°æ®
            doc_name: æ–‡æ¡£åç§°
        """
        if not isinstance(json_data, dict):
            return

        # ä¸ºåŸºç¡€å®ä½“æ·»åŠ æ–‡æ¡£æ¥æº
        if "åŸºç¡€å®ä½“" in json_data and isinstance(json_data["åŸºç¡€å®ä½“"], list):
            for entity in json_data["åŸºç¡€å®ä½“"]:
                if isinstance(entity, dict):
                    entity["æ–‡æ¡£æ¥æº"] = doc_name

        # ä¸ºçŠ¶æ€å®ä½“æ·»åŠ æ–‡æ¡£æ¥æº
        if "çŠ¶æ€å®ä½“" in json_data and isinstance(json_data["çŠ¶æ€å®ä½“"], list):
            for state in json_data["çŠ¶æ€å®ä½“"]:
                if isinstance(state, dict):
                    state["æ–‡æ¡£æ¥æº"] = doc_name

        # ä¸ºçŠ¶æ€å…³ç³»æ·»åŠ æ–‡æ¡£æ¥æº
        if "çŠ¶æ€å…³ç³»" in json_data and isinstance(json_data["çŠ¶æ€å…³ç³»"], list):
            for relation in json_data["çŠ¶æ€å…³ç³»"]:
                if isinstance(relation, dict):
                    relation["æ–‡æ¡£æ¥æº"] = doc_name