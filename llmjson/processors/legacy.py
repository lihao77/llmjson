"""
å…¼å®¹æ€§é€‚é…å™¨

æ‰©å±•åŸæœ‰LLMProcessorï¼Œæ·»åŠ é€šç”¨ç³»ç»Ÿæ”¯æŒï¼ŒåŒæ—¶ä¿æŒæ‰€æœ‰åŸæœ‰åŠŸèƒ½ã€‚
"""

from typing import Dict, Any, Optional, List, Tuple
from ..processor import LLMProcessor as OriginalLLMProcessor
from ..templates.legacy import LegacyFloodTemplate
from ..validators.universal import LegacyValidatorAdapter


class EnhancedLLMProcessor(OriginalLLMProcessor):
    """å¢å¼ºç‰ˆLLMProcessorï¼Œåœ¨åŸæœ‰åŠŸèƒ½åŸºç¡€ä¸Šæ·»åŠ é€šç”¨ç³»ç»Ÿæ”¯æŒ"""
    
    def __init__(self, 
                 api_key: str,
                 base_url: str = "https://api.openai.com/v1",
                 model: str = "gpt-4o-mini",
                 temperature: float = 0.1,
                 max_tokens: int = 4000,
                 timeout: int = 60,
                 max_retries: int = 3,
                 retry_delay: float = 1.0,
                 max_workers: int = 4,
                 enable_parallel: bool = True,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 stream: bool = False,
                 force_json: bool = True,
                 extra_body: Optional[Dict[str, Any]] = None,
                 prompt_template: Optional[Any] = None,
                 word_chunker: Optional[Any] = None,
                 # æ–°å¢å‚æ•°ï¼šæ”¯æŒé€šç”¨æ¨¡æ¿
                 universal_template: Optional[Any] = None,
                 universal_validator: Optional[Any] = None):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆå¤„ç†å™¨
        
        ä¿æŒæ‰€æœ‰åŸæœ‰å‚æ•°ï¼ŒåŒæ—¶æ”¯æŒæ–°çš„é€šç”¨æ¨¡æ¿å’ŒéªŒè¯å™¨
        """
        
        # è°ƒç”¨åŸæœ‰çš„åˆå§‹åŒ–é€»è¾‘
        super().__init__(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            max_retries=max_retries,
            retry_delay=retry_delay,
            max_workers=max_workers,
            enable_parallel=enable_parallel,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            stream=stream,
            force_json=force_json,
            extra_body=extra_body,
            prompt_template=prompt_template,
            word_chunker=word_chunker
        )
        
        # æ–°å¢ï¼šé€šç”¨ç³»ç»Ÿæ”¯æŒ
        self.universal_template = universal_template
        self.universal_validator = universal_validator
        
        # æ ‡è®°æ˜¯å¦ä½¿ç”¨é€šç”¨æ¨¡å¼
        self.use_universal_mode = universal_template is not None
        
        if self.use_universal_mode:
            self.logger.info("ğŸ¯ å¯ç”¨é€šç”¨æ¨¡å¼ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿")
        else:
            self.logger.info("ğŸŒŠ ä½¿ç”¨ä¼ ç»Ÿæ´ªæ¶ç¾å®³æ¨¡å¼")
    
    def process_chunk(self, 
                     chunk: str, 
                     doc_name: str = "æœªçŸ¥æ–‡æ¡£") -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:
        """å¤„ç†æ–‡æœ¬å—
        
        å¦‚æœé…ç½®äº†é€šç”¨æ¨¡æ¿ï¼Œä½¿ç”¨é€šç”¨å¤„ç†é€»è¾‘ï¼›å¦åˆ™ä½¿ç”¨åŸæœ‰é€»è¾‘
        """
        
        if self.use_universal_mode:
            return self._process_chunk_universal(chunk, doc_name)
        else:
            # ä½¿ç”¨åŸæœ‰çš„å¤„ç†é€»è¾‘
            return super().process_chunk(chunk, doc_name)
    
    def _process_chunk_universal(self, chunk: str, doc_name: str) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:
        """ä½¿ç”¨é€šç”¨æ¨¡æ¿å¤„ç†æ–‡æœ¬å—"""
        
        try:
            # 1. ä½¿ç”¨é€šç”¨æ¨¡æ¿åˆ›å»ºæç¤º
            if self.universal_template:
                prompt = self.universal_template.create_prompt(chunk=chunk, doc_name=doc_name)
            else:
                # å›é€€åˆ°åŸæœ‰é€»è¾‘
                prompt = self._create_prompt(chunk, doc_name)
            
            # 2. è°ƒç”¨LLMï¼ˆå¤ç”¨åŸæœ‰çš„APIè°ƒç”¨é€»è¾‘ï¼‰
            reasoning, response = self._call_llm_api(prompt)
            
            # 3. æå–JSONï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
            json_data = self._extract_json(response)
            
            if json_data is None:
                return None, {
                    'success': False,
                    'error': 'JSONè§£æå¤±è´¥',
                    'mode': 'universal'
                }
            
            # 4. é€šç”¨éªŒè¯
            if self.universal_validator:
                json_data, validation_report = self.universal_validator.validate_data(json_data)
            else:
                validation_report = {"validation_skipped": True}
            
            # 5. æ·»åŠ æ–‡æ¡£æ¥æºï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
            self._add_document_source_to_json_data(json_data, doc_name)
            
            return json_data, {
                'success': True,
                'mode': 'universal',
                'validation': validation_report,
                'template_info': self.universal_template.get_template_info() if hasattr(self.universal_template, 'get_template_info') else {}
            }
            
        except Exception as e:
            return None, {
                'success': False,
                'error': str(e),
                'mode': 'universal'
            }
    
    def set_universal_template(self, template, validator=None):
        """åŠ¨æ€è®¾ç½®é€šç”¨æ¨¡æ¿"""
        self.universal_template = template
        self.universal_validator = validator
        self.use_universal_mode = template is not None
        
        mode = "é€šç”¨æ¨¡å¼" if self.use_universal_mode else "æ´ªæ¶ç¾å®³æ¨¡å¼"
        self.logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°{mode}")
    
    def get_mode_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å¼ä¿¡æ¯"""
        return {
            'mode': 'universal' if self.use_universal_mode else 'flood_disaster',
            'template_type': type(self.universal_template).__name__ if self.universal_template else 'PromptTemplate',
            'validator_type': type(self.universal_validator).__name__ if self.universal_validator else 'DataValidator',
            'original_features_available': True
        }


# ä¸ºäº†ä¿æŒå®Œå…¨å…¼å®¹ï¼Œæä¾›ä¸€ä¸ªåˆ«å
LegacyProcessorAdapter = EnhancedLLMProcessor