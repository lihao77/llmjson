"""
é€šç”¨å¤„ç†å™¨

åŸºäºæ¨¡æ¿å’ŒéªŒè¯å™¨çš„é€šç”¨ä¿¡æ¯æŠ½å–å¤„ç†å™¨ã€‚
"""

from typing import Dict, Any, Optional, List, Tuple
import time
import json
import re
import jsonschema
from openai import OpenAI
import json_repair

from ..templates.base import BaseTemplate
from ..validators.base import BaseValidator
from ..log import create_logger_with_context, log_execution_time
from ..exceptions import LLMProcessingError, APIConnectionError


class UniversalProcessor:
    """é€šç”¨å¤„ç†å™¨ï¼Œæ”¯æŒä»»æ„é¢†åŸŸçš„ä¿¡æ¯æŠ½å–"""
    
    def __init__(self, 
                 template: BaseTemplate,
                 validator: Optional[BaseValidator] = None,
                 api_key: str = None,
                 base_url: str = "https://api.openai.com/v1",
                 model: str = "gpt-4o-mini",
                 temperature: float = 0.1,
                 max_tokens: int = 4000,
                 timeout: int = 60,
                 max_retries: int = 3,
                 retry_delay: float = 1.0,
                 **kwargs):
        """åˆå§‹åŒ–é€šç”¨å¤„ç†å™¨
        
        Args:
            template: æ¨¡æ¿å®ä¾‹
            validator: éªŒè¯å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            api_key: OpenAI APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        self.logger = create_logger_with_context({
            'component': 'UniversalProcessor',
            'model': model,
            'template': template.__class__.__name__
        })
        
        self.template = template
        self.validator = validator
        
        # LLMé…ç½®
        if api_key:
            self.client = OpenAI(api_key=api_key, base_url=base_url)
        else:
            self.client = None
            self.logger.warning("æœªæä¾›APIå¯†é’¥ï¼Œå°†æ— æ³•è°ƒç”¨LLM")
        
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_used': 0,
            'json_parsing_errors': 0
        }
    
    @log_execution_time()
    def process_chunk(self, chunk: str, doc_name: str = "æœªçŸ¥æ–‡æ¡£") -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:
        """å¤„ç†æ–‡æœ¬å—ï¼Œç”Ÿæˆç»“æ„åŒ–æ•°æ®
        
        Args:
            chunk: å¾…å¤„ç†çš„æ–‡æœ¬å—
            doc_name: æ–‡æ¡£åç§°
            
        Returns:
            (å¤„ç†ç»“æœ, å¤„ç†ä¿¡æ¯)
            
        Raises:
            LLMProcessingError: å½“å¤„ç†å¤±è´¥æ—¶
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        # åˆ›å»ºå¤„ç†ç‰¹å®šçš„æ—¥å¿—å™¨
        process_logger = create_logger_with_context({
            'operation': 'process_chunk',
            'doc_name': doc_name
        })
        
        try:
            process_logger.debug(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£å—ï¼Œé•¿åº¦: {len(chunk)} å­—ç¬¦")
            
            # 1. åˆ›å»ºæç¤º
            prompt = self.template.create_prompt(chunk=chunk, doc_name=doc_name)
            process_logger.debug(f"ğŸ“ æç¤ºåˆ›å»ºå®Œæˆï¼Œæ¶ˆæ¯æ•°: {len(prompt)}")
            
            # 2. è°ƒç”¨LLM API
            if not self.client:
                raise LLMProcessingError("æœªé…ç½®LLMå®¢æˆ·ç«¯")
            
            response = self._call_llm_api(prompt)
            process_logger.debug(f"ğŸ“¡ APIè°ƒç”¨å®Œæˆï¼Œå“åº”é•¿åº¦: {len(response) if response else 0} å­—ç¬¦")
            
            # 3. æå–JSONæ•°æ®
            json_data = self._extract_json(response)
            if json_data is None:
                self.stats['json_parsing_errors'] += 1
                self.stats['failed_requests'] += 1
                
                error_details = {
                    'success': False,
                    'error': 'JSONè§£æå¤±è´¥',
                    'error_type': 'json_parse_error',
                    'raw_response': response[:1000] if response else None,
                    'processing_time': time.time() - start_time,
                    'chunk_length': len(chunk)
                }
                
                process_logger.error(f"âŒ JSONè§£æå¤±è´¥")
                return None, error_details
            
            # 4. æ¨¡æ¿éªŒè¯
            try:
                jsonschema.validate(json_data, self.template.schema)
                process_logger.debug(f"âœ… æ¨¡æ¿éªŒè¯é€šè¿‡")
            except jsonschema.ValidationError as e:
                self.stats['failed_requests'] += 1
                error_details = {
                    'success': False,
                    'error': 'è¾“å‡ºæ ¼å¼ä¸ç¬¦åˆæ¨¡æ¿è¦æ±‚',
                    'error_type': 'template_validation_error',
                    'validation_error': str(e),
                    'validation_path': list(e.absolute_path) if e.absolute_path else [],
                    'failed_value': e.instance,
                    'schema_path': list(e.schema_path) if e.schema_path else [],
                    'raw_output': response[:2000] if response else None,
                    'processing_time': time.time() - start_time
                }
                process_logger.error(f"âŒ æ¨¡æ¿éªŒè¯å¤±è´¥: {str(e)}")
                process_logger.error(f"   éªŒè¯è·¯å¾„: {error_details['validation_path']}")
                process_logger.error(f"   å¤±è´¥å€¼: {error_details['failed_value']}")
                return None, error_details
            
            # 5. æ•°æ®éªŒè¯å’Œä¿®æ­£
            validation_result = {"validation_skipped": True}
            if self.validator:
                json_data, validation_result = self.validator.validate_data(json_data)
                process_logger.debug(f"âœ… æ•°æ®éªŒè¯å®Œæˆ")
            
            # 6. æ·»åŠ æ–‡æ¡£æ¥æº
            self._add_document_source(json_data, doc_name)
            
            processing_time = time.time() - start_time
            self.stats['successful_requests'] += 1
            
            success_details = {
                'success': True,
                'model': self.model,
                'chunk_length': len(chunk),
                'response_length': len(response) if response else 0,
                'processing_time': processing_time,
                'validation': validation_result,
                'template_info': self.template.get_template_info() if hasattr(self.template, 'get_template_info') else {}
            }
            
            process_logger.info(f"âœ… å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {processing_time:.2f}s")
            
            return json_data, success_details
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.stats['failed_requests'] += 1
            error_msg = f"å¤„ç†æ–‡æœ¬å—å¤±è´¥: {str(e)}"
            
            process_logger.error(f"âŒ {error_msg}")
            
            raise LLMProcessingError(error_msg) from e
    
    def _call_llm_api(self, prompt: List[Dict[str, str]]) -> str:
        """è°ƒç”¨LLM API
        
        Args:
            prompt: messages åˆ—è¡¨
            
        Returns:
            LLMå“åº”æ–‡æœ¬
            
        Raises:
            APIConnectionError: å½“APIè°ƒç”¨å¤±è´¥æ—¶
        """
        # åˆ›å»ºAPIè°ƒç”¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡æ—¥å¿—å™¨
        api_logger = create_logger_with_context({
            'operation': 'api_call',
        })
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": self.model,
            "messages": prompt,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "timeout": self.timeout,
            "response_format": {"type": "json_object"}
        }
        
        # é‡è¯•æœºåˆ¶
        last_exception = None
        for attempt in range(self.max_retries):
            try:
                api_logger.info(f"ğŸ“¡ å¼€å§‹APIè°ƒç”¨ (å°è¯• {attempt + 1}/{self.max_retries})")
                
                response = self.client.chat.completions.create(**request_params)
                
                # è®°å½•å“åº”ä¿¡æ¯
                if hasattr(response, 'usage') and response.usage:
                    self.stats['total_tokens_used'] += response.usage.total_tokens
                    api_logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸ!")
                    api_logger.info(f"  ğŸ“¥ è¾“å…¥Token: {response.usage.prompt_tokens}")
                    api_logger.info(f"  ğŸ“¤ è¾“å‡ºToken: {response.usage.completion_tokens}")
                    api_logger.info(f"  ğŸ“Š æ€»Token: {response.usage.total_tokens}")
                
                response_content = response.choices[0].message.content
                api_logger.info(f"  ğŸ“ å“åº”é•¿åº¦: {len(response_content) if response_content else 0} å­—ç¬¦")
                
                return response_content
                    
            except Exception as e:
                last_exception = e
                api_logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)}")
                
                if attempt < self.max_retries - 1:
                    sleep_time = self.retry_delay * (2 ** attempt)
                    api_logger.info(f"â³ ç­‰å¾… {sleep_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
                    continue
                else:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        error_msg = f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{self.max_retries}æ¬¡: {str(last_exception)}"
        api_logger.error(f"âŒ {error_msg}")
        raise APIConnectionError(error_msg) from last_exception
    
    def _extract_json(self, response: str) -> Optional[Dict[str, Any]]:
        """ä»LLMå“åº”ä¸­æå–JSONæ•°æ®
        
        Args:
            response: LLMå“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„JSONæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not response:
            return None
        
        # åˆ›å»ºJSONæå–ç‰¹å®šçš„æ—¥å¿—å™¨
        extract_logger = create_logger_with_context({
            'operation': 'json_extraction'
        })
        
        extract_logger.debug(f"ğŸ” å¼€å§‹JSONæå–ï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        
        # 1. é¦–å…ˆå°è¯•ç›´æ¥è§£æ
        try:
            result = json.loads(response)
            extract_logger.info("âœ… ç›´æ¥JSONè§£ææˆåŠŸ")
            return result
        except json.JSONDecodeError as e:
            extract_logger.debug(f"âŒ ç›´æ¥JSONè§£æå¤±è´¥: {str(e)}")
        
        # 2. å°è¯•æå–JSONä»£ç å—
        extract_logger.debug("ğŸ” å°è¯•æå–JSONä»£ç å—...")
        json_patterns = [
            r'```(?:json)?\s*({.*?})\s*```',  # æ ‡å‡†ä»£ç å—
            r'```(?:json)?\s*(\[.*?\])\s*```'  # æ•°ç»„ä»£ç å—
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            for i, match in enumerate(matches):
                try:
                    result = json.loads(match)
                    extract_logger.info(f"âœ… JSONä»£ç å—è§£ææˆåŠŸ (ç¬¬{i+1}ä¸ª)")
                    return result
                except json.JSONDecodeError:
                    continue
        
        # 3. æ™ºèƒ½æŸ¥æ‰¾JSONå¯¹è±¡
        extract_logger.debug("ğŸ” å°è¯•æ™ºèƒ½JSONå¯¹è±¡æå–...")
        json_candidates = self._find_json_candidates(response)
        
        for i, candidate in enumerate(json_candidates):
            try:
                result = json.loads(candidate)
                extract_logger.info(f"âœ… JSONå¯¹è±¡è§£ææˆåŠŸ (å€™é€‰é¡¹{i+1})")
                return result
            except json.JSONDecodeError:
                continue
        
        # 4. ä½¿ç”¨json_repairå°è¯•ä¿®å¤
        extract_logger.debug("ğŸ”§ å°è¯•JSONä¿®å¤...")
        try:
            repaired = json_repair.repair_json(response)
            result = json.loads(repaired)
            extract_logger.info(f"âœ… JSONä¿®å¤æˆåŠŸ")
            return result
        except Exception as e:
            extract_logger.debug(f"âŒ JSONä¿®å¤å¤±è´¥: {str(e)}")
        
        extract_logger.error("âŒ æ‰€æœ‰JSONæå–æ–¹æ³•éƒ½å¤±è´¥äº†")
        return None
    
    def _find_json_candidates(self, text: str) -> List[str]:
        """æ™ºèƒ½æŸ¥æ‰¾JSONå€™é€‰é¡¹"""
        candidates = []
        
        # æŸ¥æ‰¾å®Œæ•´çš„å¤§æ‹¬å·åŒ…å›´çš„å†…å®¹
        brace_count = 0
        start_pos = -1
        
        for i, char in enumerate(text):
            if char == '{':
                if brace_count == 0:
                    start_pos = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_pos != -1:
                    candidate = text[start_pos:i + 1]
                    if len(candidate) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                        candidates.append(candidate)
                    start_pos = -1
        
        return candidates[:5]  # é™åˆ¶å€™é€‰é¡¹æ•°é‡
    
    def _add_document_source(self, json_data: Dict[str, Any], doc_name: str) -> None:
        """ä¸ºJSONæ•°æ®æ·»åŠ æ–‡æ¡£æ¥æºä¿¡æ¯"""
        if not isinstance(json_data, dict):
            return
        
        # é€’å½’æ·»åŠ æ–‡æ¡£æ¥æº
        def add_source_recursive(obj, source):
            if isinstance(obj, dict):
                obj["æ–‡æ¡£æ¥æº"] = source
                for value in obj.values():
                    if isinstance(value, (dict, list)):
                        add_source_recursive(value, source)
            elif isinstance(obj, list):
                for item in obj:
                    if isinstance(item, (dict, list)):
                        add_source_recursive(item, source)
        
        # ä¸ºé¡¶çº§æ•°ç»„æ·»åŠ æ¥æº
        for key, value in json_data.items():
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, dict):
                        item["æ–‡æ¡£æ¥æº"] = doc_name
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        # è®¡ç®—æˆåŠŸç‡
        if stats['total_requests'] > 0:
            stats['success_rate'] = stats['successful_requests'] / stats['total_requests'] * 100
        else:
            stats['success_rate'] = 0.0
        
        # è®¡ç®—å¹³å‡tokenä½¿ç”¨é‡
        if stats['successful_requests'] > 0:
            stats['avg_tokens_per_request'] = stats['total_tokens_used'] / stats['successful_requests']
        else:
            stats['avg_tokens_per_request'] = 0.0
        
        return stats